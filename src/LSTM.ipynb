{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "Long Short-Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "# Dataset\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "# from google_drive_downloader import GoogleDriveDownloader as gdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioDataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_dataset = \"short_audio_dataset\"\n",
    "sliced_dataset_lenght = 16050\n",
    "# sliced_dataset = \"shorter_audio_dataset\"\n",
    "# sliced_dataset_lenght = 4013\n",
    "original_dataset = \"audio_dataset\"\n",
    "original_dataset_lenght = 80249\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, root_path=\"./data/\", drop_both=False, use_short=False, normalize=False, use_features=False):\n",
    "        root_folder = root_path + original_dataset if not use_short else root_path + sliced_dataset\n",
    "        self.use_features = use_features\n",
    "        self.max_length = original_dataset_lenght if not use_short else sliced_dataset_lenght\n",
    "        self.class_map = {\"esben\" : 0, \"peter\": 1, \"both\": 2}\n",
    "        self.data = []\n",
    "        self.wavs = []\n",
    "        self.labels = []\n",
    "        self.features_list = []\n",
    "        self.min_val = 10e10\n",
    "        self.max_val = 0\n",
    "        print(\"Start reading files\")\n",
    "        for subdir, dirs, files in os.walk(root_folder):\n",
    "            for file_name in files:\n",
    "                if drop_both and \"both\" in subdir:\n",
    "                   continue\n",
    "        \n",
    "                file_path = os.path.join(subdir, file_name)\n",
    "                self.sample_rate, wav = wavfile.read(file_path)\n",
    "                wav = wav.astype(np.float32)\n",
    "                \n",
    "                if wav.shape[0] > self.max_length:\n",
    "                    self.max_length = wav.shape[0]\n",
    "                    print(\"Found wav with more length than specified max one, new max is:\", wav.shape[0])\n",
    "                \n",
    "                wav = np.pad(wav, (0, self.max_length-wav.shape[0]))\n",
    "                features = np.transpose(self.feature_extraction(wav, self.sample_rate, normalize=normalize))\n",
    "                label_str = file_path.split('/')[-3][2:]\n",
    "                label = (np.int64(self.class_map[label_str]))\n",
    "                \n",
    "                self.max_val = np.max(wav) if np.max(wav) > self.max_val else self.max_val\n",
    "                self.min_val = np.min(wav) if np.min(wav) < self.min_val else self.min_val\n",
    "                \n",
    "                self.wavs.append(wav)\n",
    "                self.features_list.append(features)\n",
    "                self.labels.append(label)\n",
    "               \n",
    "        self.wavs = np.array(self.wavs)\n",
    "        self.mu  = self.wavs.mean()\n",
    "        self.std = np.std(self.wavs)\n",
    "        # self.wavs = torch.Tensor(self.wavs)\n",
    "        if normalize:\n",
    "            self.wavs = (self.wavs + np.abs(self.min_val)) / (np.abs(self.min_val) + self.max_val)\n",
    "            # self.wavs = torch.nn.functional.normalize(self.wavs, dim=1)\n",
    "        \n",
    "        print(\"=\"*40)\n",
    "        print(\"Loaded DATABASE from {}\\n{} total file\\nLongest file is {} long\\nMean: {}\\nStandard deviation: {}\\nNormalization: {}\".\n",
    "              format(root_folder, len(self.wavs), self.max_length, self.mu, self.std, normalize))\n",
    "        if use_features:\n",
    "            print(\"Feature dimensions for the first few samples:\")\n",
    "            for i in range(5):  # Print dimensions for the first 5 samples as an example\n",
    "                print(\"Sample {}: {}\".format(i, self.features_list[i].shape))\n",
    "        print(\"=\"*40)\n",
    "    \n",
    "    def feature_extraction(self, wav, sample_rate, n_mfcc=128, normalize=False):\n",
    "        # extract features from the audio\n",
    "        mfcc = np.mean(librosa.feature.mfcc(y=wav, sr=sample_rate, n_mfcc=n_mfcc).T, axis=0)\n",
    "        if normalize:\n",
    "            mfcc = (mfcc + np.abs(np.min(mfcc))) / (np.abs(np.min(mfcc)) + np.max(mfcc))\n",
    "        return mfcc\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wavs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        wav = self.wavs[idx]\n",
    "        label = self.labels[idx]\n",
    "        wav_tensor = torch.from_numpy(wav)\n",
    "        label_tensor = torch.Tensor(label)\n",
    "        if self.use_features:\n",
    "            features = self.features_list[idx]\n",
    "            features_tensor = torch.Tensor(features)\n",
    "            return features_tensor, label_tensor\n",
    "        return wav_tensor, label_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start reading files\n",
      "========================================\n",
      "Loaded DATABASE from ../data/short_audio_dataset\n",
      "1000 total file\n",
      "Longest file is 16050 long\n",
      "Mean: -0.6988561153411865\n",
      "Standard deviation: 2332.388427734375\n",
      "Normalization: True\n",
      "Feature dimensions for the first few samples:\n",
      "Sample 0: (128,)\n",
      "Sample 1: (128,)\n",
      "Sample 2: (128,)\n",
      "Sample 3: (128,)\n",
      "Sample 4: (128,)\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "audio_dataset = AudioDataset(root_path=\"../data/\", drop_both=True, use_short=True, normalize=True, use_features=True)\n",
    "dataset_len = len(audio_dataset)\n",
    "train_size, test_size, valid_size = int(dataset_len * 0.7), int(dataset_len * 0.2), int(dataset_len * 0.1)\n",
    "\n",
    "dataset_train, dataset_test, dataset_valid = torch.utils.data.random_split(audio_dataset, (train_size, test_size, valid_size))\n",
    "\n",
    "kwargs = {'batch_size': 1, 'num_workers': 2}\n",
    "loader_train = torch.utils.data.DataLoader(dataset_train, **kwargs, shuffle=True)\n",
    "loader_test = torch.utils.data.DataLoader(dataset_test, **kwargs, shuffle=True)\n",
    "loader_valid = torch.utils.data.DataLoader(dataset_valid, **kwargs, shuffle=True)\n",
    "\n",
    "# 33% should be considered like with the other methods \n",
    "# In the solution he plots the lengths of the data...but thats just for educational purpose so I didn't reuse that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**\n",
    "\n",
    "* Create a class to load the dataset\n",
    "    * To be able to batch emails, they have to have the same length. You can ensure this in different ways e.g. zero-padding\n",
    "    * Make a training and validation set based on *emails_ascii* and *targets*  \n",
    "* Create and train a recurrent model to classify whether the emails are spam or not\n",
    "    * Have a look at the [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) module and the [Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to batch-process the mails even though some of the mails are shorter than the chosen text-length, the dataset zero-pads the text and returns an index for the last valid character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class voiceClassifier(nn.Module):\n",
    "\n",
    "#    def __init__(self, d=512, n_lstm_layers=1, dropout=0.):\n",
    "#        super().__init__()\n",
    "#        self.emb = nn.Embedding(128, d)  # max(emails_ascii) < 128 -------- WHAT DOES IT NEED TO BE FOR US?\n",
    "#        self.lstm = nn.LSTM(d, d, n_lstm_layers, batch_first=True, dropout=dropout)\n",
    "#        self.lin = nn.Linear(d, 1)\n",
    "\n",
    "#    def forward(self, x, end_idx):  # (B, nx)\n",
    "#        x = self.emb(x)  # (B, nx, d)\n",
    "#        y = self.lstm(x)[0][torch.arange(len(x)), end_idx]  # (B, d)\n",
    "#        return self.lin(y).view(-1)  # (B,)\n",
    "class voiceClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(voiceClassifier, self).__init__()  # Use voiceClassifier instead of AudioRNN\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()  # For binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # Output shape: (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        output = self.sigmoid(out)  # Sigmoid for binary classification\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c036602495c8493f95001985f18c681f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[-3449, -3397, -3408,  ...,     0,     0,     0],\n",
      "        [ 1871,  2118,  2216,  ...,     0,     0,     0],\n",
      "        [  821,   442,   120,  ...,     0,     0,     0],\n",
      "        [   -1,    -3,    -5,  ...,     0,     0,     0],\n",
      "        [ -602,  -253,  -305,  ...,     0,     0,     0]])\n",
      "end_idx: tensor([80248, 80248, 80248, 80248, 80248])\n",
      "x: torch.Size([5, 80249])\n",
      "end_idx: torch.Size([5])\n",
      "y: torch.Size([5])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_idx:\u001b[39m\u001b[38;5;124m\"\u001b[39m, end_idx\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m---> 33\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(logits, y\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     36\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Documents/3Semester/DNN/Identity-recognition-from-audio/dnn_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/3Semester/DNN/Identity-recognition-from-audio/dnn_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[26], line 27\u001b[0m, in \u001b[0;36mvoiceClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Forward propagate LSTM\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Output shape: (batch_size, seq_length, hidden_size)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Decode the hidden state of the last time step\u001b[39;00m\n\u001b[1;32m     30\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n",
      "File \u001b[0;32m~/Documents/3Semester/DNN/Identity-recognition-from-audio/dnn_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/3Semester/DNN/Identity-recognition-from-audio/dnn_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/3Semester/DNN/Identity-recognition-from-audio/dnn_venv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:871\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    869\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor unbatched 2-D input, hx and cx should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    870\u001b[0m                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malso be 2-D but got (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D) tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 871\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    872\u001b[0m     hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    873\u001b[0m \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors"
     ]
    }
   ],
   "source": [
    "#device = torch.device('cuda')\n",
    "\n",
    "#model = voiceClassifier().to(device)\n",
    "input_size = 128  # I think this is the size of our features\n",
    "hidden_size = 128  # Choose an appropriate size\n",
    "num_layers = 1  # Number of LSTM layers\n",
    "output_size = 1  # For binary classification\n",
    "model = voiceClassifier(input_size, hidden_size, num_layers, output_size)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "sched = torch.optim.lr_scheduler.MultiStepLR(opt, (30,))\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "lrs = []\n",
    "\n",
    "pbar = tqdm(range(40))\n",
    "for epoch in pbar:\n",
    "    # train\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct = total = 0\n",
    "    for x, end_idx, y in train_loader:\n",
    "        x, end_idx, y = x.type(torch.LongTensor), end_idx.type(torch.LongTensor), y.type(torch.LongTensor)\n",
    "        \n",
    "        print(\"x:\", x)\n",
    "        \n",
    "        print(\"end_idx:\", end_idx)\n",
    "        print(\"x:\", x.size())\n",
    "        print(\"end_idx:\", end_idx.size())\n",
    "        print(\"y:\", y.size())\n",
    "        logits = model(x)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, y.float())\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        correct += ((torch.sigmoid(logits) > 0.5) == y).sum().item()\n",
    "        total += len(x)\n",
    "    train_loss = np.mean(losses)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # valid\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct = total = 0\n",
    "    for x, end_idx, y in loader_valid:\n",
    "        x, end_idx, y = x.to(device), end_idx.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x, end_idx)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, y.float())\n",
    "        losses.append(loss.item())\n",
    "        correct += ((torch.sigmoid(logits) > 0.5) == y).sum().item()\n",
    "        total += len(x)\n",
    "    valid_loss = np.mean(losses)\n",
    "    valid_acc = correct / total\n",
    "\n",
    "    # sched\n",
    "    sched.step()\n",
    "\n",
    "    # history\n",
    "    lrs.append(next(iter(opt.param_groups))['lr'])\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    valid_accuracies.append(valid_acc)\n",
    "\n",
    "    pbar.set_description(f'loss: {train_loss:.3f}/{valid_loss:.3f}, acc: {train_acc:.2f}/{valid_acc:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot history\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 3))\n",
    "axs[0].plot(train_losses, label='train')\n",
    "axs[0].plot(valid_losses, labeFile ~/Documents/3Semester/DNN/Identity-recognition-from-audio/dnn_venv/lib/python3.10/site-packages/torch/nn/functional.py:2233, in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\n",
    "   2227     # Note [embedding_renorm set_grad_enabled]\n",
    "   2228     # XXX: equivalent to\n",
    "   2229     # with torch.no_grad():\n",
    "   2230     #   torch.embedding_renorm_\n",
    "   2231     # remove once script supports set_grad_enabled\n",
    "   2232     _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n",
    "-> 2233 return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
    "\n",
    "IndexError: index out of range in selfl='valid')\n",
    "axs[0].set_ylabel('loss')\n",
    "axs[0].legend()\n",
    "axs[1].plot(train_accuracies, label='train')\n",
    "axs[1].plot(valid_accuracies, label='valid')\n",
    "axs[1].set_ylabel('acc')\n",
    "axs[1].set_ylim(0.8, 1)\n",
    "axs[1].legend()\n",
    "axs[2].plot(lrs)\n",
    "axs[2].set_ylabel('lr')\n",
    "axs[2].set_yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
