{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "Long Short-Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "# Dataset\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "# from google_drive_downloader import GoogleDriveDownloader as gdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioDataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_dataset = \"../data/short_audio_dataset\"\n",
    "sliced_dataset_lenght = 16050\n",
    "original_dataset = \"../data/audio_dataset\"\n",
    "original_dataset_lenght = 80249\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, drop_both=False):\n",
    "        root_folder = original_dataset\n",
    "        max_length = original_dataset_lenght\n",
    "        self.class_map = {\"esben\" : 0, \"peter\": 1, \"both\": 2}\n",
    "        self.data = []\n",
    "        self.wavs = []\n",
    "        self.labels = []\n",
    "        for subdir, dirs, files in os.walk(root_folder):\n",
    "            for file_name in files:\n",
    "                if \"both\" in subdir and drop_both:\n",
    "                   continue\n",
    "                file_path = os.path.join(subdir, file_name)\n",
    "                _, wav = wavfile.read(file_path)\n",
    "                wav = wav.astype(np.float32)\n",
    "                if wav.shape[0] > max_length:\n",
    "                    max_length = wav.shape[0]\n",
    "                    print(\"Found wav with more length than specified max one, new max is:\", wav.shape[0])\n",
    "                wav = np.pad(wav, (0, max_length-wav.shape[0]))\n",
    "                label_str = file_path.split('/')[3][2:]\n",
    "                label = (np.int64(self.class_map[label_str]))\n",
    "                self.wavs.append(wav)\n",
    "                self.labels.append(label)\n",
    "                self.data.append([wav, label])\n",
    "        print(\"Max length of wav files:\", max_length)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav = self.wavs[idx]\n",
    "        class_id = self.labels[idx]\n",
    "        # class_id = self.class_map[label]\n",
    "        #wav_tensor = torch.from_numpy(wav)\n",
    "        #class_id = torch.tensor(class_id)\n",
    "        return wav, (len(wav) - 1), class_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of wav files: 80249\n",
      "200\n",
      "150 30 20\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "dataset = AudioDataset(drop_both=True)\n",
    "print(len(dataset.data))\n",
    "train_data, val_data, test_data = random_split (dataset, [150, 30, 20])\n",
    "print(len(train_data), len(val_data), len(test_data))\n",
    "train_loader = DataLoader(train_data, batch_size=5, shuffle=True)\n",
    "valid_loader = DataLoader(val_data, batch_size=5, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=5, shuffle=True)\n",
    "\n",
    "# 33% should be considered like with the other methods \n",
    "# In the solution he plots the lengths of the data...but thats just for educational purpose so I didn't reuse that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**\n",
    "\n",
    "* Create a class to load the dataset\n",
    "    * To be able to batch emails, they have to have the same length. You can ensure this in different ways e.g. zero-padding\n",
    "    * Make a training and validation set based on *emails_ascii* and *targets*  \n",
    "* Create and train a recurrent model to classify whether the emails are spam or not\n",
    "    * Have a look at the [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) module and the [Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to batch-process the mails even though some of the mails are shorter than the chosen text-length, the dataset zero-pads the text and returns an index for the last valid character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class voiceClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, d=512, n_lstm_layers=1, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(128, d)  # max(emails_ascii) < 128 -------- WHAT DOES IT NEED TO BE FOR US?\n",
    "        self.lstm = nn.LSTM(d, d, n_lstm_layers, batch_first=True, dropout=dropout)\n",
    "        self.lin = nn.Linear(d, 1)\n",
    "\n",
    "    def forward(self, x, end_idx):  # (B, nx)\n",
    "        x = self.emb(x)  # (B, nx, d)\n",
    "        y = self.lstm(x)[0][torch.arange(len(x)), end_idx]  # (B, d)\n",
    "        return self.lin(y).view(-1)  # (B,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c197576c4604837ac18e6d46e07cfa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[  437,   459,   439,  ...,    56,    44,     0],\n",
      "        [ 1690,   115, -2135,  ...,   134,   134,     0],\n",
      "        [   81,   589,   744,  ...,     0,     0,     0],\n",
      "        [-2589, -3260, -3546,  ..., -4841, -5530,     0],\n",
      "        [ -941, -1327, -1920,  ...,    39,    43,     0]])\n",
      "end_idx: tensor([80248, 80248, 80248, 80248, 80248])\n",
      "y: tensor([1, 0, 0, 0, 0])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_idx:\u001b[39m\u001b[38;5;124m\"\u001b[39m, end_idx)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y)\n\u001b[0;32m---> 25\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(logits, y\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     28\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Documents/3Semester/DNN/Identity-recognition-from-audio/dnn_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/3Semester/DNN/Identity-recognition-from-audio/dnn_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[78], line 10\u001b[0m, in \u001b[0;36mvoiceClassifier.forward\u001b[0;34m(self, x, end_idx)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, end_idx):  \u001b[38;5;66;03m# (B, nx)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, nx, d)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)[\u001b[38;5;241m0\u001b[39m][torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(x)), end_idx]  \u001b[38;5;66;03m# (B, d)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin(y)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/3Semester/DNN/Identity-recognition-from-audio/dnn_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/3Semester/DNN/Identity-recognition-from-audio/dnn_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/3Semester/DNN/Identity-recognition-from-audio/dnn_venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/3Semester/DNN/Identity-recognition-from-audio/dnn_venv/lib/python3.10/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "#device = torch.device('cuda')\n",
    "\n",
    "#model = voiceClassifier().to(device)\n",
    "model = voiceClassifier()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "sched = torch.optim.lr_scheduler.MultiStepLR(opt, (30,))\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "lrs = []\n",
    "\n",
    "pbar = tqdm(range(40))\n",
    "for epoch in pbar:\n",
    "    # train\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct = total = 0\n",
    "    for x, end_idx, y in train_loader:\n",
    "        x, end_idx, y = x.type(torch.LongTensor), end_idx.type(torch.LongTensor), y.type(torch.LongTensor)\n",
    "        print(\"x:\", x)\n",
    "        print(\"end_idx:\", end_idx)\n",
    "        print(\"y:\", y)\n",
    "        logits = model(x, end_idx)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, y.float())\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        correct += ((torch.sigmoid(logits) > 0.5) == y).sum().item()\n",
    "        total += len(x)\n",
    "    train_loss = np.mean(losses)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # valid\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct = total = 0\n",
    "    for x, end_idx, y in loader_valid:\n",
    "        x, end_idx, y = x.to(device), end_idx.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x, end_idx)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, y.float())\n",
    "        losses.append(loss.item())\n",
    "        correct += ((torch.sigmoid(logits) > 0.5) == y).sum().item()\n",
    "        total += len(x)\n",
    "    valid_loss = np.mean(losses)\n",
    "    valid_acc = correct / total\n",
    "\n",
    "    # sched\n",
    "    sched.step()\n",
    "\n",
    "    # history\n",
    "    lrs.append(next(iter(opt.param_groups))['lr'])\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    valid_accuracies.append(valid_acc)\n",
    "\n",
    "    pbar.set_description(f'loss: {train_loss:.3f}/{valid_loss:.3f}, acc: {train_acc:.2f}/{valid_acc:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot history\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 3))\n",
    "axs[0].plot(train_losses, label='train')\n",
    "axs[0].plot(valid_losses, labeFile ~/Documents/3Semester/DNN/Identity-recognition-from-audio/dnn_venv/lib/python3.10/site-packages/torch/nn/functional.py:2233, in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\n",
    "   2227     # Note [embedding_renorm set_grad_enabled]\n",
    "   2228     # XXX: equivalent to\n",
    "   2229     # with torch.no_grad():\n",
    "   2230     #   torch.embedding_renorm_\n",
    "   2231     # remove once script supports set_grad_enabled\n",
    "   2232     _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n",
    "-> 2233 return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
    "\n",
    "IndexError: index out of range in selfl='valid')\n",
    "axs[0].set_ylabel('loss')\n",
    "axs[0].legend()\n",
    "axs[1].plot(train_accuracies, label='train')\n",
    "axs[1].plot(valid_accuracies, label='valid')\n",
    "axs[1].set_ylabel('acc')\n",
    "axs[1].set_ylim(0.8, 1)\n",
    "axs[1].legend()\n",
    "axs[2].plot(lrs)\n",
    "axs[2].set_ylabel('lr')\n",
    "axs[2].set_yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
