{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-nearest neigbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.io import wavfile\n",
    "\n",
    "# sklearn\n",
    "from sklearn import neighbors\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "#Sklearn\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AudioDataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_dataset = \"../data/short_audio_dataset\"\n",
    "sliced_dataset_lenght = 16050\n",
    "original_dataset = \"../data/audio_dataset\"\n",
    "original_dataset_lenght = 80249\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, drop_both=False):\n",
    "        root_folder = original_dataset\n",
    "        max_length = original_dataset_lenght\n",
    "        self.class_map = {\"both\": 0, \"esben\" : 1, \"peter\": 2}\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        for subdir, dirs, files in os.walk(root_folder):\n",
    "            for file_name in files:\n",
    "                if \"both\" in subdir and drop_both:\n",
    "                   continue\n",
    "                file_path = os.path.join(subdir, file_name)\n",
    "                _, wav = wavfile.read(file_path)\n",
    "                if wav.shape[0] > max_length:\n",
    "                    max_length = wav.shape[0]\n",
    "                    print(\"Found wav with more length than specified max one, new max is:\", wav.shape[0])\n",
    "                wav = np.pad(wav, (0, max_length-wav.shape[0]))\n",
    "                label = file_path.split('/')[3][2:]\n",
    "                self.labels.append(label)\n",
    "                self.data.append(wav)\n",
    "        print(\"Max length of wav files:\", max_length)\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        wav = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        class_id = self.class_map[label]\n",
    "        wav_tensor = torch.from_numpy(wav)\n",
    "        class_id = torch.tensor([class_id])\n",
    "        return wav_tensor, class_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of wav files: 80249\n"
     ]
    }
   ],
   "source": [
    "dataset = AudioDataset()\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(dataset.data, dataset.labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define knn algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_param_search(train_data, train_labels, test_data, test_labels, \n",
    "                     metrics=('manhattan', 'euclidean', 'chebyshev'), \n",
    "                     ks=(1, 3, 5, 10, 25, 50, 100, 250), \n",
    "                     n_train=None, n_test=None, algorithm='brute'):\n",
    "  \"\"\"\n",
    "  Takes a dataset and plots knn classification accuracy \n",
    "  for different hyper parameters.\n",
    "\n",
    "  n_train and n_test allows to subsample the dataset for faster iteration\n",
    "  \"\"\"\n",
    "  x_train = np.array(train_data)\n",
    "  y_train = np.array(train_labels)\n",
    "  x_test = np.array(test_data)\n",
    "  y_test = np.array(test_labels)\n",
    "  \n",
    "  # subsample the dataset\n",
    "  if n_train:\n",
    "    x_train, y_train = x_train[:n_train], y_train[:n_train]\n",
    "  if n_test:\n",
    "    x_test, y_test = x_test[:n_test], y_test[:n_test]\n",
    "\n",
    "  for metric in metrics:\n",
    "    print(f'Metric: {metric}')\n",
    "    for k in ks:\n",
    "        print(f'\\tk: {k:3d} Training', end='')\n",
    "        classifier = neighbors.KNeighborsClassifier(k, algorithm=algorithm, metric=metric)\n",
    "        classifier = classifier.fit(x_train, y_train)\n",
    "\n",
    "        start = time.time()\n",
    "        print(f'\\r\\tk: {k:3d} Testing', end='')\n",
    "        labels = classifier.predict(x_test)\n",
    "        duration = time.time() - start\n",
    "\n",
    "        correct = labels == np.array(y_test)\n",
    "        print(f'\\r\\tk: {k:3d} Accuracy: {correct.mean() * 100:.2f} %, Duration: {duration:.2f} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Call the KNN Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: manhattan\n",
      "\tk:   2 Accuracy: 40.00 %, Duration: 0.77 s\n",
      "Metric: euclidean\n",
      "\tk:   2 Accuracy: 36.67 %, Duration: 0.10 s\n",
      "Metric: chebyshev\n",
      "\tk:   2 Accuracy: 36.67 %, Duration: 0.62 s\n"
     ]
    }
   ],
   "source": [
    "knn_param_search(train_data, \n",
    "                 train_labels, \n",
    "                 val_data, \n",
    "                 val_labels,\n",
    "                 ks=[3]\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_nearest_neighbours(train_data, train_labels, test_data, test_labels,\n",
    "                            n_examples=20, n_neighbours=10, seed=None, \n",
    "                            scale=1., labelnames=tuple(range(10))):\n",
    "  x_train = np.array(train_data)\n",
    "  y_train = np.array(train_labels)\n",
    "  x_test = np.array(test_data)\n",
    "  y_test = np.array(test_labels)\n",
    "\n",
    "  rand = np.random.RandomState(seed=seed)\n",
    "  idx = rand.randint(0, len(x_test), n_examples)\n",
    "  x_test, y_test = x_test[idx], y_test[idx].reshape(-1)\n",
    "  y_train = y_train.reshape(-1)\n",
    "\n",
    "  d_data = np.prod(x_test.shape[1:])\n",
    "  classifier = neighbors.KNeighborsClassifier(algorithm='brute', metric='euclidean')\n",
    "  classifier = classifier.fit(x_train.reshape((-1, d_data)), y_train)\n",
    "  dist, idx = classifier.kneighbors(x_test[:n_examples].reshape(-1, d_data), \n",
    "                                    n_neighbors=n_neighbours)\n",
    "\n",
    "  fig, axs = plt.subplots(n_neighbours + 1, n_examples, \n",
    "                          figsize=(n_examples * 0.5 * scale, \n",
    "                                   n_neighbours * 0.75 * scale))\n",
    "  cmap = 'gray' if len(x_test.shape) == 3 else None\n",
    "  for i in range(n_examples):\n",
    "    ax = axs[0, i]\n",
    "    ax.imshow(x_test[i], cmap=cmap)\n",
    "    ax.set_title(labelnames[y_test[i]])\n",
    "    if i == 0:\n",
    "      ax.set_ylabel('inp')\n",
    "    for j in range(n_neighbours):\n",
    "      ax = axs[j + 1, i]\n",
    "      n_idx = idx[i, j]\n",
    "      y = y_train[n_idx]\n",
    "      ax.imshow(x_train[n_idx], cmap=cmap)\n",
    "      ax.set_title(labelnames[y] if y != y_test[i] else '')\n",
    "      if i == 0:\n",
    "        ax.set_ylabel(f'n{j+1}')\n",
    "  for ax in axs.reshape(-1):\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "  plt.tight_layout()\n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_nearest_neighbours(mnist_train_dataset.data, mnist_train_dataset.targets, \n",
    "                        mnist_test_dataset.data, mnist_test_dataset.targets, \n",
    "                        seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the nearest neighbours, it becomes apparent that for many of the test images, an almost identical image is in the training set.\n",
    "\n",
    "To us, the nearest neighbour 'mistakes' (n1-n10 with another marked number) may seem odd, but remember that the distance is meassured in pixel space.\n",
    "\n",
    "Let's have a look at what kind of mistakes the model makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(gt, pred, only_show_mistakes=False, \n",
    "                          labelnames='auto'):\n",
    "  gt = np.array(gt)\n",
    "  pred = np.array(pred)\n",
    "  if only_show_mistakes:\n",
    "    mask = gt != pred\n",
    "    gt, pred = gt[mask], pred[mask]\n",
    "\n",
    "  conf_matrix = confusion_matrix(gt, pred)\n",
    "  plt.figure(figsize=(10, 8))\n",
    "  ax = seaborn.heatmap(conf_matrix / conf_matrix.sum(), annot=True, fmt='.2%', \n",
    "                       cmap='Blues', cbar=False, yticklabels=labelnames)\n",
    "  ax.set_ylabel('ground truth')\n",
    "  ax.set_xlabel('predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test, labels, only_show_mistakes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mistake_examples(x_test, y_test, labels, labelnames=tuple(range(10)), \n",
    "                          rows=4, cols=7, seed=None):\n",
    "  x_test = np.array(x_test)\n",
    "  y_test = np.array(y_test)\n",
    "  labels = np.array(labels)\n",
    "  mask = labels != y_test\n",
    "  x_test, y_test, labels = x_test[mask], y_test[mask], labels[mask]\n",
    "  idx = np.random.RandomState(seed=seed).randint(0, len(x_test), rows * cols)\n",
    "  x_test, y_test, labels = x_test[idx], y_test[idx], labels[idx]\n",
    "  cmap = 'gray' if x_test.ndim == 3 else None\n",
    "  fig, axs = plt.subplots(rows, cols, figsize=(cols, rows * 1.0))\n",
    "  for i, ax in enumerate(axs.reshape(-1)):\n",
    "    ax.imshow(x_test[i], cmap=cmap)\n",
    "    gt, pred = labelnames[y_test[i]], labelnames[labels[i]]\n",
    "    ax.set_title(f'{gt} ({pred})')\n",
    "    ax.axis('off')\n",
    "  plt.tight_layout()\n",
    "  print('ground truth (predicted)')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mistake_examples(x_test, y_test, np.array(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_train_dataset = datasets.CIFAR10('../', train=True, download=True)\n",
    "cifar10_test_dataset = datasets.CIFAR10('../', train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_classes = cifar10_train_dataset.classes\n",
    "for i in range(9):\n",
    "  plt.subplot(3,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(cifar10_test_dataset.data[i], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(cifar10_classes[cifar10_test_dataset.targets[i]]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_param_search(cifar10_train_dataset.data, cifar10_train_dataset.targets, \n",
    "                 cifar10_test_dataset.data, cifar10_test_dataset.targets,\n",
    "                 ['euclidean'], n_test=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train) = (cifar10_train_dataset.data, cifar10_train_dataset.targets)\n",
    "(x_test, y_test) = (cifar10_test_dataset.data, cifar10_test_dataset.targets)\n",
    "\n",
    "# Use all data to evaluate performance of the most promising hyperparameters\n",
    "classifier = neighbors.KNeighborsClassifier(10, algorithm='brute', metric='euclidean')\n",
    "classifier = classifier.fit(x_train.reshape((-1, 32 * 32 * 3)), y_train)\n",
    "labels = classifier.predict(x_test.reshape((-1, 32 * 32 * 3)))\n",
    "correct = labels == np.array(y_test)\n",
    "print(f'Accuracy: {correct.mean() * 100} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mistake_examples(x_test, y_test, labels, labelnames=cifar10_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't work nearly as well as our MNIST classifier. Some of the predictions seem ridiculous. Let's look at the nearest neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_nearest_neighbours(cifar10_train_dataset.data, cifar10_train_dataset.targets,\n",
    "                        cifar10_test_dataset.data, cifar10_test_dataset.targets,\n",
    "                        n_examples=10, n_neighbours=5, scale=2, labelnames=cifar10_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could improve accuracy by gathering more data, but that's expensive.\n",
    "Also note that inference time grows with the amount of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of measuring the distance directly in pixel-space,\n",
    "we can attempt to extract features using a classical decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "d_pca = 20  # hyper parameter\n",
    "pca = PCA(n_components=d_pca).fit(x_train.reshape((-1, 32 * 32 * 3)))\n",
    "x_train_pca = pca.transform(x_train.reshape((-1, 32 * 32 * 3)))\n",
    "x_test_pca = pca.transform(x_test.reshape((-1, 32 * 32 * 3)))\n",
    "print(f'\\tCIFAR-10 compressed train x/y shape: {x_train_pca.shape} / {y_train.shape}')\n",
    "print(f'\\tCIFAR-10 compressed test x/y shape: {x_test_pca.shape} / {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can visualize the pca components\n",
    "fig, axs = plt.subplots(4, 5, figsize=(5, 5))\n",
    "for i, ax in enumerate(axs.reshape(-1)):\n",
    "  pci = pca.components_[i].reshape(32, 32, 3)\n",
    "  ax.imshow(0.5 + pci * 0.5 / np.abs(pci).max())\n",
    "  ax.set_title(f'PC{i}')\n",
    "  ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_param_search(x_train_pca, y_train, x_test_pca, y_test, n_test=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = neighbors.KNeighborsClassifier(25, algorithm='brute', metric='manhattan')\n",
    "classifier = classifier.fit(x_train_pca, y_train)\n",
    "labels = classifier.predict(x_test_pca)\n",
    "correct = labels == y_test\n",
    "print(f'Accuracy: {correct.mean() * 100} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
