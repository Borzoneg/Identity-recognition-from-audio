{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "rng = np.random.default_rng(123456)\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import copy\n",
    "from scipy.io import wavfile\n",
    "import random\n",
    "\n",
    "import os\n",
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioDataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_dataset = \"short_audio_dataset\"\n",
    "sliced_dataset_lenght = 16050\n",
    "# sliced_dataset = \"shorter_audio_dataset\"\n",
    "# sliced_dataset_lenght = 4013\n",
    "original_dataset = \"audio_dataset\"\n",
    "original_dataset_lenght = 80249\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, root_path=\"./data/\", drop_both=False, use_short=False, active='wavs', normalize=False):\n",
    "        root_folder = root_path + original_dataset if not use_short else root_path + sliced_dataset\n",
    "        self.max_length = original_dataset_lenght if not use_short else sliced_dataset_lenght\n",
    "        self.class_map = {\"esben\" : 0, \"peter\": 1, \"both\": 2}\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.min_val = 10e10\n",
    "        self.max_val = 0\n",
    "        self.normalize = normalize\n",
    "        self.wavs, self.mfccs, self.chromas, self.contrasts, self.centroids, self.bandwidths = [], [], [], [], [], []\n",
    "        self.wavs_norm, self.mfccs_norm, self.chromas_norm, self.contrasts_norm, self.centroids_norm, self.bandwidths_norm = [], [], [], [], [], []\n",
    "        \n",
    "        print(\"Start reading files and genearting features\")\n",
    "        for subdir, dirs, files in os.walk(root_folder):\n",
    "            for file_name in files:\n",
    "                if drop_both and \"both\" in subdir:\n",
    "                   continue\n",
    "        \n",
    "                file_path = os.path.join(subdir, file_name)\n",
    "                self.sample_rate, wav = wavfile.read(file_path)\n",
    "                wav = wav.astype(np.float32)\n",
    "                \n",
    "                if wav.shape[0] > self.max_length:\n",
    "                    self.max_length = wav.shape[0]\n",
    "                    print(\"Found wav with more length than specified max one, new max is:\", wav.shape[0])\n",
    "                \n",
    "                self.feature_extraction(wav, self.sample_rate)\n",
    "                wav = np.pad(wav, (0, self.max_length-wav.shape[0]))\n",
    "                label_str = file_path.split('/')[-3][2:]\n",
    "                label = (np.int64(self.class_map[label_str]))\n",
    "                \n",
    "                self.max_val = np.max(wav) if np.max(wav) > self.max_val else self.max_val\n",
    "                self.min_val = np.min(wav) if np.min(wav) < self.min_val else self.min_val\n",
    "                \n",
    "                self.wavs.append(wav)\n",
    "                self.labels.append(label)\n",
    "               \n",
    "        self.wavs = np.array(self.wavs)\n",
    "        self.mu  = self.wavs.mean()\n",
    "        self.std = np.std(self.wavs)\n",
    "        # self.wavs = torch.Tensor(self.wavs)\n",
    "\n",
    "        self.active = active\n",
    "        self.values_dict = {'wavs': 0, 'mfcc': 1, 'chroma': 2, 'contrast': 3, 'centroid': 4, 'bandwidth': 5}\n",
    "        self.values_list = [self.wavs, self.mfccs, self.chromas, self.contrasts, self.centroids, self.bandwidths]\n",
    "        self.values_norm_list = []\n",
    "        print(\"Generating normalized arrays\")\n",
    "        for lst in self.values_list:\n",
    "            self.values_norm_list.append((lst + np.abs(np.min(lst))) / (np.abs(np.min(lst)) + np.max(lst)))\n",
    "            \n",
    "        print(\"=\"*40)\n",
    "        print(\"Loaded DATABASE from {}\\n{} total file\\nLongest file is {} long\\nMean: {}\\nStandard deviation: {}\\n\".\n",
    "              format(root_folder, len(self.wavs), self.max_length, self.mu, self.std))\n",
    "        print(\"=\"*40)\n",
    "\n",
    "    def feature_extraction(self, wav, sample_rate):\n",
    "        self.mfccs.append(np.transpose(np.mean(librosa.feature.mfcc(y=wav, sr=sample_rate, n_mfcc=128).T, axis=0)))\n",
    "        # self.chromas.append(np.transpose(np.mean(librosa.feature.chroma_cqt(y=wav, sr=sample_rate).T, axis=0)))\n",
    "        self.chromas.append(np.transpose(np.mean(librosa.feature.chroma_stft(y=wav, sr=sample_rate).T, axis=0)))\n",
    "        self.contrasts.append(np.transpose(np.mean(librosa.feature.spectral_contrast(y=wav, sr=sample_rate).T, axis=0)))\n",
    "        self.centroids.append(np.transpose(np.mean(librosa.feature.spectral_centroid(y=wav, sr=sample_rate).T, axis=0)))\n",
    "        self.bandwidths.append(np.transpose(np.mean(librosa.feature.spectral_bandwidth(y=wav, sr=sample_rate).T, axis=0)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wavs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        y = self.labels[idx]\n",
    "        x = self.values_list[self.values_dict[self.active]][idx]\n",
    "        if self.normalize:\n",
    "            x = self.values_norm_list[self.values_dict[self.active]][idx]\n",
    "        x = torch.Tensor(x)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start reading files and genearting features\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m audio_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mAudioDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_both\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_short\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m dataset_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(audio_dataset)\n\u001b[1;32m      3\u001b[0m train_size, test_size, valid_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(dataset_len \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.7\u001b[39m), \u001b[38;5;28mround\u001b[39m(dataset_len \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.2\u001b[39m), \u001b[38;5;28mround\u001b[39m(dataset_len \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 35\u001b[0m, in \u001b[0;36mAudioDataset.__init__\u001b[0;34m(self, root_path, drop_both, use_short, active, normalize)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m=\u001b[39m wav\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound wav with more length than specified max one, new max is:\u001b[39m\u001b[38;5;124m\"\u001b[39m, wav\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m wav \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpad(wav, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;241m-\u001b[39mwav\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     37\u001b[0m label_str \u001b[38;5;241m=\u001b[39m file_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m][\u001b[38;5;241m2\u001b[39m:]\n",
      "Cell \u001b[0;32mIn[12], line 65\u001b[0m, in \u001b[0;36mAudioDataset.feature_extraction\u001b[0;34m(self, wav, sample_rate)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeature_extraction\u001b[39m(\u001b[38;5;28mself\u001b[39m, wav, sample_rate):\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmfccs\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mtranspose(np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmfcc\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwav\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mfcc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)))\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# self.chromas.append(np.transpose(np.mean(librosa.feature.chroma_cqt(y=wav, sr=sample_rate).T, axis=0)))\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchromas\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mtranspose(np\u001b[38;5;241m.\u001b[39mmean(librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mchroma_stft(y\u001b[38;5;241m=\u001b[39mwav, sr\u001b[38;5;241m=\u001b[39msample_rate)\u001b[38;5;241m.\u001b[39mT, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/librosa/feature/spectral.py:1989\u001b[0m, in \u001b[0;36mmfcc\u001b[0;34m(y, sr, S, n_mfcc, dct_type, norm, lifter, **kwargs)\u001b[0m\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Mel-frequency cepstral coefficients (MFCCs)\u001b[39;00m\n\u001b[1;32m   1844\u001b[0m \n\u001b[1;32m   1845\u001b[0m \u001b[38;5;124;03m.. warning:: If multi-channel audio input ``y`` is provided, the MFCC\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1985\u001b[0m \u001b[38;5;124;03m>>> fig.colorbar(img2, ax=[ax[1]])\u001b[39;00m\n\u001b[1;32m   1986\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m S \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1988\u001b[0m     \u001b[38;5;66;03m# multichannel behavior may be different due to relative noise floor differences between channels\u001b[39;00m\n\u001b[0;32m-> 1989\u001b[0m     S \u001b[38;5;241m=\u001b[39m power_to_db(\u001b[43mmelspectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1991\u001b[0m M: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39mfftpack\u001b[38;5;241m.\u001b[39mdct(S, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mdct_type, norm\u001b[38;5;241m=\u001b[39mnorm)[\n\u001b[1;32m   1992\u001b[0m     \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :n_mfcc, :\n\u001b[1;32m   1993\u001b[0m ]\n\u001b[1;32m   1995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lifter \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1996\u001b[0m     \u001b[38;5;66;03m# shape lifter for broadcasting\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/librosa/feature/spectral.py:2143\u001b[0m, in \u001b[0;36mmelspectrogram\u001b[0;34m(y, sr, S, n_fft, hop_length, win_length, window, center, pad_mode, power, **kwargs)\u001b[0m\n\u001b[1;32m   2130\u001b[0m S, n_fft \u001b[38;5;241m=\u001b[39m _spectrogram(\n\u001b[1;32m   2131\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m   2132\u001b[0m     S\u001b[38;5;241m=\u001b[39mS,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2139\u001b[0m     pad_mode\u001b[38;5;241m=\u001b[39mpad_mode,\n\u001b[1;32m   2140\u001b[0m )\n\u001b[1;32m   2142\u001b[0m \u001b[38;5;66;03m# Build a Mel filter\u001b[39;00m\n\u001b[0;32m-> 2143\u001b[0m mel_basis \u001b[38;5;241m=\u001b[39m \u001b[43mfilters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2145\u001b[0m melspec: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...ft,mf->...mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, S, mel_basis, optimize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m melspec\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/librosa/filters.py:252\u001b[0m, in \u001b[0;36mmel\u001b[0;34m(sr, n_fft, n_mels, fmin, fmax, htk, norm, dtype)\u001b[0m\n\u001b[1;32m    249\u001b[0m     weights \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mnormalize(weights, norm\u001b[38;5;241m=\u001b[39mnorm, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# Only check weights if f_mel[0] is positive\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall((mel_f[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m|\u001b[39m (\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)):\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;66;03m# This means we have an empty channel somewhere\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmpty filters detected in mel frequency basis. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome channels will produce empty responses. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    259\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    260\u001b[0m     )\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weights\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/_methods.py:39\u001b[0m, in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     33\u001b[0m     _complex_to_float\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[1;32m     34\u001b[0m         nt\u001b[38;5;241m.\u001b[39mdtype(nt\u001b[38;5;241m.\u001b[39mclongdouble) : nt\u001b[38;5;241m.\u001b[39mdtype(nt\u001b[38;5;241m.\u001b[39mlongdouble),\n\u001b[1;32m     35\u001b[0m     })\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# avoid keyword arguments to speed up parsing, saves about 15%-20% for very\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# small reductions\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_amax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_maximum(a, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out, keepdims, initial, where)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_amin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     44\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "audio_dataset = AudioDataset(root_path=\"../data/\", drop_both=False, use_short=False)\n",
    "dataset_len = len(audio_dataset)\n",
    "train_size, test_size, valid_size = round(dataset_len * 0.7), round(dataset_len * 0.2), round(dataset_len * 0.1)\n",
    "\n",
    "dataset_train, dataset_test, dataset_valid = torch.utils.data.random_split(audio_dataset, (train_size, test_size, valid_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'batch_size': 1, 'num_workers': 1}\n",
    "loader_train = torch.utils.data.DataLoader(dataset_train, **kwargs, shuffle=True)\n",
    "loader_test = torch.utils.data.DataLoader(dataset_test, **kwargs, shuffle=True)\n",
    "loader_valid = torch.utils.data.DataLoader(dataset_valid, **kwargs, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size, test_size = round(dataset_len * 0.8), round(dataset_len * 0.2)\n",
    "dataset_train, dataset_test = torch.utils.data.random_split(audio_dataset, (train_size, test_size))\n",
    "kwargs = {'batch_size': 1, 'num_workers': 1}\n",
    "loader_train = torch.utils.data.DataLoader(dataset_train, **kwargs, shuffle=True)\n",
    "loader_test = torch.utils.data.DataLoader(dataset_test, **kwargs, shuffle=True)\n",
    "train_data = []\n",
    "train_labels = []\n",
    "test_data = []\n",
    "test_labels = []\n",
    "lists_data = [train_data, test_data]\n",
    "lists_labels = [train_labels, test_labels]\n",
    "\n",
    "i = 0\n",
    "for loader in [loader_train, loader_test]:\n",
    "    for x, y in loader:\n",
    "        lists_data[i].append(np.array(x[0]))\n",
    "        lists_labels[i].append(np.array(y[0]))\n",
    "    i += 1\n",
    "for li in lists_data:\n",
    "    print(len(li))\n",
    "for li in lists_labels:\n",
    "    print(len(li))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dataset.active = 'wavs'\n",
    "audio_dataset.normalize = False\n",
    "\n",
    "for x, y in loader_test:\n",
    "    plt.plot(np.arange(x.shape[1]), x.flatten())\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_len):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_len, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, input_len),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.decode(self.encode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(len(dataset_train[0][0]))\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(25):\n",
    "    print(f'Epoch {epoch+1:02}/25', end=' ')\n",
    "    for x, _ in loader_train:  \n",
    "        x_rec = model(x) \n",
    "        loss = F.binary_cross_entropy(x_rec, x)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        mse = F.mse_loss(x_rec, x)\n",
    "        mae = F.l1_loss(x_rec, x)\n",
    "        \n",
    "    print(f'loss: {loss.item():.4f} - rmse: {np.sqrt(mse.item()):.4f} - mae: {mae.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_param_search(train_data, train_labels, test_data, test_labels,\n",
    "                     metrics=('manhattan', 'euclidean', 'chebyshev'), \n",
    "                     ks=(1, 3, 5, 10, 25, 50, 100, 250), \n",
    "                     n_train=None, n_test=None, algorithm='brute'):\n",
    "  \"\"\"\n",
    "  Takes a dataset and plots knn classification accuracy \n",
    "  for different hyper parameters.\n",
    "\n",
    "  n_train and n_test allows to subsample the dataset for faster iteration\n",
    "  \"\"\"\n",
    "  x_train = np.array(train_data)\n",
    "  y_train = np.array(train_labels)\n",
    "  x_test = np.array(test_data)\n",
    "  y_test = np.array(test_labels)\n",
    "  max_acc = 0\n",
    "  datas = []\n",
    "  for metric in metrics:\n",
    "    for k in ks:\n",
    "        print(f'Metric: {metric}; k: {k:3};', end=' ')\n",
    "        classifier = neighbors.KNeighborsClassifier(k, algorithm=algorithm, metric=metric)\n",
    "        classifier = classifier.fit(x_train, y_train)\n",
    "\n",
    "        labels = classifier.predict(x_test)\n",
    "        \n",
    "        correct = labels == np.array(y_test)\n",
    "        print(f'Accuracy: {correct.mean() * 100:.2f}%')\n",
    "        if correct.mean() > max_acc:\n",
    "          max_acc = correct.mean()\n",
    "          best_classifier = classifier\n",
    "          best_metric = metric\n",
    "          best_k = k\n",
    "        datas.append([metric, k, correct.mean()])\n",
    "      \n",
    "  print(f'Best classifier | metric: {best_metric}; k: {best_k:3}; accuracy: {max_acc * 100:.2f}%')\n",
    "  return best_classifier, datas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/knn_results.txt', 'w') as f:\n",
    "  for norm_opt in [False, True]:\n",
    "    audio_dataset.normalize = norm_opt\n",
    "    for feature in ['wavs', 'mfcc', 'chroma', 'contrast']:\n",
    "      audio_dataset.active = feature\n",
    "      \n",
    "      train_data = []\n",
    "      train_labels = []\n",
    "      test_data = []\n",
    "      test_labels = []\n",
    "      lists_data = [train_data, test_data]\n",
    "      lists_labels = [train_labels, test_labels]\n",
    "      i = 0\n",
    "      for loader in [loader_train, loader_test]:\n",
    "          for x, y in loader:\n",
    "              lists_data[i].append(np.array(x[0]))\n",
    "              lists_labels[i].append(np.array(y[0]))\n",
    "          i += 1\n",
    "\n",
    "      classifier, datas = knn_param_search(train_data, \n",
    "                                    train_labels, \n",
    "                                    test_data,\n",
    "                                    test_labels,\n",
    "                                  #   metrics=['euclidean'],\n",
    "                                    ks=(1, 3, 5, 10, 25, 50, 100))\n",
    "      f.write(f'Norm:{norm_opt};feature:{feature};{datas}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old plot KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_norm_no_feat = [43.00, 50.00, 50.00, 50.00, 51.00, 51.00, 50.00, 50.00, 50.00, 47.00, 49.00, 50.00, 50.00, 50.00, 50.00, 50.00, 45.00, 48.00, 49.00, 54.00, 50.00, 53.00, 54.00, 52.00]\n",
    "norm_no_feat = [50.00, 49.00, 58.00, 57.00, 59.00, 54.00, 57.00, 59.00, 52.00, 54.00, 57.00, 57.00, 45.00, 57.00, 57.00, 57.00, 46.00, 45.00, 47.00, 44.00, 51.00, 59.00, 59.00, 60.00]\n",
    "norm_feat = [77.00, 78.00, 72.00, 68.00, 71.00, 73.00, 70.00, 58.00, 78.00, 78.00, 76.00, 78.00, 78.00, 81.00, 76.00, 64.00, 81.00, 85.00, 84.00, 81.00, 81.00, 73.00, 74.00, 75.00]\n",
    "no_norm_feat = [88.00, 90.00, 93.00, 88.00, 84.00, 85.00, 83.00, 80.00, 83.00, 89.00, 87.00, 82.00, 83.00, 81.00, 77.00, 70.00, 77.00, 76.00, 76.00, 80.00, 76.00, 75.00, 71.00, 61.00]\n",
    "datas = [no_norm_no_feat ,norm_no_feat ,norm_feat ,no_norm_feat]\n",
    "titles = ['Norm=False; Features=False' ,'Norm=True; Features=False', 'Norm=True; Features=True', 'Norm=False; Features=True']\n",
    "ks = [1, 3, 5, 10, 25, 50, 100, 250]\n",
    "metrics = ['manhattan', 'euclidean', 'chebyshev']\n",
    "\n",
    "fig = plt.figure(figsize=(28, 16))\n",
    "for i, data in enumerate(datas):\n",
    "    # fig = plt.figure(figsize=(32,4))\n",
    "    ax = fig.add_subplot(2, 2, i+1)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    ax.title.set_text(titles[i])\n",
    "    tmp = []\n",
    "    for j in range(len(ks)):\n",
    "        tmp.append(data[j])\n",
    "        tmp.append(data[j+len(ks)])\n",
    "        tmp.append(data[j+len(ks)*2])\n",
    "        tmp.append(0)\n",
    "    tmp.pop()\n",
    "    clrs = ('blue', 'red', 'green') * len(ks)\n",
    "\n",
    "    bars = plt.bar(range(len(tmp)), tmp, width=0.8, color=clrs)\n",
    "    for bar in bars:\n",
    "        if bar.get_height() > 0:\n",
    "            ax.text(bar.get_x(), bar.get_height(), format(int(bar.get_height())))\n",
    "\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_ylabel('accuracy')\n",
    "\n",
    "    ax.set_xticks(range(len(tmp)))\n",
    "    x_tick_labels = []\n",
    "    for k in ks:\n",
    "        x_tick_labels.append('')\n",
    "        x_tick_labels.append(format(k))\n",
    "        x_tick_labels.append('')\n",
    "        x_tick_labels.append('')\n",
    "    x_tick_labels.pop()\n",
    "    ax.set_xticklabels(x_tick_labels)\n",
    "    ax.set_xlabel('K')\n",
    "\n",
    "    if i == 0:\n",
    "        legend_patches = [mpatches.Patch(color=clrs[0], label=metrics[0]), mpatches.Patch(color=clrs[1], label=metrics[1]), mpatches.Patch(color=clrs[2], label=metrics[2])]\n",
    "        plt.legend(handles=legend_patches)\n",
    "    # plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by extending nn.Module:\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self, n_hidden_neurons: int, input_len):\n",
    "    super().__init__()\n",
    "    self.fc1 = nn.Linear(input_len, n_hidden_neurons)\n",
    "    self.fc2 = nn.Linear(n_hidden_neurons, 2)\n",
    "\n",
    "  def forward(self, x: torch.Tensor):\n",
    "    # x = x.flatten(start_dim=0)  # (N, d_data)\n",
    "    h = F.relu(self.fc1(x))\n",
    "    logits = self.fc2(h)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses, all_accuracies = {}, {}\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "learning_rate = 5e-2\n",
    "n_epochs = 10\n",
    "\n",
    "for n_hidden_neurons in ([10, 100, 1000, 10000]):\n",
    "  print(f\"Hidden neurons: {n_hidden_neurons}\")\n",
    "  model = MLP(n_hidden_neurons, len(dataset_train[0][0]))\n",
    "  opt = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  train_losses, train_accuracies = [], []\n",
    "  valid_losses, valid_accuracies = [], []\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "    print(f\"{epoch+1:02} / 10 Epoch\", end=\": \")\n",
    "\n",
    "    epoch_losses = []\n",
    "    correct, total = 0, 0\n",
    "    for x, y in loader_train:\n",
    "      opt.zero_grad()\n",
    "  \n",
    "      logits = model(x)  # logits: common name for the output before softmax activation\n",
    "  \n",
    "      log_probs = F.log_softmax(logits, dim=1)  # numerically stable version of log(softmax(logits))\n",
    "  \n",
    "      loss = F.nll_loss(log_probs, y)  # negative log likelihood loss\n",
    "      # or just: loss = F.cross_entropy(logits, y)\n",
    "\n",
    "      loss.backward()\n",
    "      opt.step()\n",
    "      epoch_losses.append(loss.item())\n",
    "      total += len(x)\n",
    "      correct += (torch.argmax(logits, dim=1) == y).sum().item()\n",
    "    train_losses.append(np.mean(epoch_losses))\n",
    "    train_accuracies.append(correct / total)\n",
    "    print(f\"Train; Loss mean: {np.mean(epoch_losses):2.2f}; Accuracy: {correct / total:2.2f}\", end=\" | \")\n",
    "    \n",
    "    epoch_losses = []\n",
    "    correct, total = 0, 0\n",
    "    for x, y in loader_valid:\n",
    "      with torch.no_grad():\n",
    "        logits = model(x)\n",
    "      loss = F.cross_entropy(logits, y)\n",
    "\n",
    "      epoch_losses.append(loss.item())\n",
    "      total += len(x)\n",
    "      correct += (torch.argmax(logits, dim=1) == y).sum().item()\n",
    "    valid_losses.append(np.mean(epoch_losses))\n",
    "    valid_accuracy = correct / total\n",
    "    valid_accuracies.append(valid_accuracy)\n",
    "    print(f\"Valid; Loss mean: {np.mean(epoch_losses):2.2f}; Accuracy: {valid_accuracy:2.2f}\")\n",
    "    if valid_accuracy > best_accuracy:\n",
    "      best_accuracy = valid_accuracy\n",
    "      best_model = copy.deepcopy(model), n_hidden_neurons, epoch\n",
    "\n",
    "  all_losses[n_hidden_neurons] = train_losses, valid_losses\n",
    "  all_accuracies[n_hidden_neurons] = train_accuracies, valid_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results from MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "for n, (train_losses, valid_losses) in all_losses.items():\n",
    "  p = plt.plot(train_losses, label=f'{n}:train')\n",
    "  plt.plot(valid_losses, label=f'{n}:valid', ls='--', c=p[0].get_color())\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(0, 1)\n",
    "for n, (train_accuracies, valid_accuracies) in all_accuracies.items():\n",
    "  p = plt.plot(train_accuracies, label=f'{n}:train')\n",
    "  plt.plot(valid_accuracies, label=f'{n}:valid', ls='--', c=p[0].get_color())\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "model, n_hidden_neurons, epoch = best_model\n",
    "print(f'best val accuracy: {best_accuracy:.2f} with {n_hidden_neurons} hidden neurons after {epoch} epoch')\n",
    "\n",
    "correct, total = 0, 0\n",
    "for x, y in loader_valid:\n",
    "  with torch.no_grad():\n",
    "    logits = model(x)\n",
    "  total += len(x)\n",
    "  correct += (torch.argmax(logits, dim=1) == y).sum().item()\n",
    "print(f'test accuracy: {correct / total:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM MIMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineClassifier(nn.Module):\n",
    "    def __init__(self, input_size, n_lstm_layers=1, dropout=0.):\n",
    "        super().__init__()\n",
    "        # self.emb = nn.Embedding(128, d)  # max(emails_ascii) < 128\n",
    "        self.lstm = nn.LSTM(input_size, input_size, n_lstm_layers, batch_first=False, dropout=dropout)\n",
    "        self.lin = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x, end_idx):  # (B, nx)\n",
    "        # x = self.emb(x)  # (B, nx, d)\n",
    "        # y = self.lstm(x)[0][torch.arange(len(x)), end_idx]  # (B, d)\n",
    "        y = self.lstm(x)\n",
    "        return y\n",
    "        # return self.lin(y).view(-1)  # (B,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "model = OnlineClassifier(audio_dataset.max_length).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "sched = torch.optim.lr_scheduler.MultiStepLR(opt, (30,))\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "lrs = []\n",
    "epochs = 40\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"{epoch+1:02} / 10 Epoch\", end=\": \")\n",
    "    # train\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct = total = 0\n",
    "    for x, end_idx, y in loader_train:\n",
    "        x, end_idx, y = x.to(device), end_idx.to(device), y.to(device)\n",
    "        logits = model(x, end_idx)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, y.float())\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        correct += ((torch.sigmoid(logits) > 0.5) == y).sum().item()\n",
    "        total += len(x)\n",
    "    train_loss = np.mean(losses)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # valid\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct = total = 0\n",
    "    for x, end_idx, y in loader_valid:\n",
    "        x, end_idx, y = x.to(device), end_idx.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x, end_idx)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, y.float())\n",
    "        losses.append(loss.item())\n",
    "        correct += ((torch.sigmoid(logits) > 0.5) == y).sum().item()\n",
    "        total += len(x)\n",
    "    valid_loss = np.mean(losses)\n",
    "    valid_acc = correct / total\n",
    "\n",
    "    # sched\n",
    "    sched.step()\n",
    "    \n",
    "    # history\n",
    "    lrs.append(next(iter(opt.param_groups))['lr'])\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    valid_accuracies.append(valid_acc)\n",
    "\n",
    "    print(f'loss: {train_loss:.3f}/{valid_loss:.3f}, acc: {train_acc:.2f}/{valid_acc:.2f}')\n",
    "\n",
    "# plot history\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 3))\n",
    "axs[0].plot(train_losses, label='train')\n",
    "axs[0].plot(valid_losses, label='valid')\n",
    "axs[0].set_ylabel('loss')\n",
    "axs[0].legend()\n",
    "axs[1].plot(train_accuracies, label='train')\n",
    "axs[1].plot(valid_accuracies, label='valid')\n",
    "axs[1].set_ylabel('acc')\n",
    "axs[1].set_ylim(0.8, 1)\n",
    "axs[1].legend()\n",
    "axs[2].plot(lrs)\n",
    "axs[2].set_ylabel('lr')\n",
    "axs[2].set_yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot from KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_function_knn(file_name, folder_name=None, showing=True):\n",
    "    if not showing:\n",
    "        print(\"Plotting is off\")\n",
    "        plt.ioff()\n",
    "    if folder_name == None:\n",
    "        folder_name = '../output/' + file_name.split('/')[-1].replace('.txt', '')\n",
    "    folder_name += '_' + str(round(random.random()*1e4)) + '/'    \n",
    "    width = 0.8\n",
    "    if 'long' in file_name:\n",
    "        ks=(1, 3, 5, 10, 25, 50, 100)\n",
    "    else:\n",
    "        ks=(1, 3, 5, 10, 25, 50, 100, 250)\n",
    "    metrics = ['manhattan', 'euclidean', 'chebyshev']\n",
    "    color_dict = {'manhattan':'firebrick', 'euclidean':'dodgerblue', 'chebyshev':'seagreen'}\n",
    "    os.mkdir(folder_name)\n",
    "\n",
    "    with open(file_name) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            norm_str = line.split(';')[0]\n",
    "            feature_str = line.split(';')[1]\n",
    "            data_list = line.split(';')[2].replace('[', '').replace(']', '').replace('\\'', '').replace(' ', '').split(',')\n",
    "            fig = plt.figure(figsize=(10, 5))\n",
    "            fig.suptitle(norm_str + ' ' + feature_str)\n",
    "            ax = fig.add_subplot()\n",
    "\n",
    "            fig.tight_layout()\n",
    "            i = 0\n",
    "            x_pos = {1: 0, 3: 3, 5: 6, 10: 9, 25: 12, 50: 15, 100: 18, 250:21}\n",
    "            while i < (len(data_list) -2):\n",
    "                metric = data_list[i]\n",
    "                k = int(data_list[i+1])\n",
    "                acc = float(data_list[i+2])\n",
    "                i += 3\n",
    "                plt.bar(x_pos[k], acc-0.01, width=width, color=color_dict[metric])\n",
    "                ax.text(x_pos[k]-0.5, acc, '{0:.2f}'.format(acc))\n",
    "                x_pos[k] += 1\n",
    "                \n",
    "            x_tick_labels = []\n",
    "            for k in ks:\n",
    "                x_tick_labels.append('')\n",
    "                x_tick_labels.append(format(k))\n",
    "                x_tick_labels.append('')\n",
    "                \n",
    "            ax.set_yticks(np.arange(0, 1, 0.1))\n",
    "            ax.set_xticks(range(len(x_tick_labels)))\n",
    "            ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "            ax.set_ylabel('accuracy')\n",
    "            ax.set_xlabel('K')\n",
    "\n",
    "            legend_patches = []\n",
    "            for metric in color_dict:\n",
    "                legend_patches.append(mpatches.Patch(color=color_dict[metric], label=metric))\n",
    "            plt.legend(handles=legend_patches)\n",
    "            plt.savefig(folder_name + norm_str.replace(':', '_') + '_' + feature_str.replace(':', '_') + '.png')\n",
    "            if showing:\n",
    "                plt.show()\n",
    "            plt.clf()\n",
    "    if not showing:\n",
    "        plt.ion()\n",
    "    plt.close(\"all\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting function LSTM/MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_function_lstm_mlp(file_name, folder_name=None, showing=True):\n",
    "    if not showing:\n",
    "        print(\"Plotting is off\")\n",
    "        plt.ioff()\n",
    "    if folder_name == None:\n",
    "        folder_name = '../output/' + file_name.split('/')[-1].replace('.txt', '')\n",
    "    folder_name += '_' + str(round(random.random()*1e4)) + '/'\n",
    "    test_acc = {}\n",
    "    os.mkdir(folder_name)\n",
    "    with open(file_name) as  f:\n",
    "        lines = f.readlines()\n",
    "        for i in range(len(lines)):\n",
    "            line_list = lines[i].split(\":\")\n",
    "            if len(line_list) == 3:\n",
    "                fig = plt.figure(figsize=(8, 4))\n",
    "                title_str = lines[i].replace('\\n', '').replace('Norm', '').replace('Feature', '').replace(':', '').replace(',', '_').replace(' ', '')\n",
    "                fig.suptitle(title_str)\n",
    "                \n",
    "                train_losses = np.array(lines[i+1].split(':')[1].replace('[', '').replace(']', '').replace(' ', '').replace('\\n', '').split(',')).astype(float)\n",
    "                train_accuracies = np.array(lines[i+2].split(':')[1].replace('[', '').replace(']', '').replace(' ', '').replace('\\n', '').split(',')).astype(float)\n",
    "                valid_losses = np.array(lines[i+3].split(':')[1].replace('[', '').replace(']', '').replace(' ', '').replace('\\n', '').split(',')).astype(float)\n",
    "                valid_accuracies = np.array(lines[i+4].split(':')[1].replace('[', '').replace(']', '').replace(' ', '').replace('\\n', '').split(',')).astype(float)\n",
    "                test_acc[lines[i]] = float(lines[i+5].split(':')[1])\n",
    "                test_acc_line =np.full((valid_losses.shape), test_acc[lines[i]])\n",
    "                i += 5\n",
    "                \n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.xlabel('epoch')\n",
    "                plt.ylabel('loss')\n",
    "                plt.plot(train_losses,  label='train', c='blue')\n",
    "                plt.plot(valid_losses,  label='valid', c='green')\n",
    "                plt.legend()\n",
    "                plt.grid()\n",
    "\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.xlabel('epoch')\n",
    "                plt.ylabel('accuracy')\n",
    "                plt.ylim(0, 1)\n",
    "                plt.yticks(np.linspace(0, 1, 10))\n",
    "                \n",
    "                plt.plot(train_accuracies, label='train', c='blue')\n",
    "                plt.plot(valid_accuracies, label='valid', c='green')\n",
    "                plt.plot(test_acc_line, label='test', ls='--', c='red')\n",
    "                plt.legend()\n",
    "                plt.grid()\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(folder_name + title_str + '.png')\n",
    "                if showing:\n",
    "                    print('showing')\n",
    "                    plt.show()\n",
    "\n",
    "        fig = plt.figure(figsize=(10,5))\n",
    "        fig.suptitle('Test accuracy comparison')\n",
    "        \n",
    "        clrs = clrs = plt.cm.rainbow(np.linspace(0, 1, 8))\n",
    "\n",
    "        ax = fig.add_subplot()\n",
    "        bars = plt.bar(range(len(test_acc)), list(test_acc.values()), hatch=['/', '/', '/', '/', '', '', '', ''], width=0.8, color=clrs)\n",
    "        \n",
    "        xlabels = []\n",
    "        for bar, feature in zip(bars, test_acc):\n",
    "            if bar.get_height() > 0:\n",
    "                feature_string = feature.replace(':','').replace(', Feature', '').replace(' ', '').replace('Norm', '').replace('False', '').replace('True', '')\n",
    "                if 'False' in feature_string:\n",
    "                    feature_string = feature_string.replace('Norm', '').replace('False', '')\n",
    "                else:\n",
    "                    feature_string = feature_string.replace('True', '')\n",
    "                ax.text(bar.get_x(), bar.get_height() + 0.01, \"{0:.5f}\".format(bar.get_height()))\n",
    "                xlabels.append(feature_string.capitalize())\n",
    "        \n",
    "        ax.set_yticks(np.arange(0, 1, 0.1))\n",
    "        ax.set_xticks(range(len(xlabels)))\n",
    "        ax.set_xticklabels(xlabels)\n",
    "        \n",
    "        legend_patches = [mpatches.Patch(hatch='///', edgecolor='black', fill=False, label='normalized')]\n",
    "        plt.legend(handles=legend_patches)\n",
    "        \n",
    "        plt.savefig(folder_name + 'test_acc.png')\n",
    "        if showing:\n",
    "            plt.show()\n",
    "        if not showing:\n",
    "            plt.ion()\n",
    "        plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting is off\n",
      "Plotting is off\n",
      "Plotting is off\n",
      "Plotting is off\n",
      "Plotting is off\n"
     ]
    }
   ],
   "source": [
    "plot_function_knn('../output/results_short_both_KNN.txt', folder_name=None, showing=False)\n",
    "plot_function_knn('../output/results_long_both_KNN.txt', folder_name=None, showing=False)\n",
    "plot_function_lstm_mlp('../output/results_short_MLP.txt', folder_name=None, showing=False)\n",
    "plot_function_lstm_mlp('../output/results_long_both_MLP.txt', folder_name=None,showing=False)\n",
    "plot_function_lstm_mlp('../output/results_long_both_LSTM.txt', folder_name=None,showing=False)\n",
    "# plot_function_lstm_mlp('../output/results_short_hyp_MLP.txt', folder_name=None, showing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_net(file_name):\n",
    "    #plt.ioff()\n",
    "    # folder_name += '_' + str(round(random.random()*1e4)) + '/'\n",
    "    test_acc = {}\n",
    "    # os.mkdir(folder_name)\n",
    "    with open(file_name) as  f:\n",
    "        lines = f.readlines()\n",
    "        for i in range(len(lines)):\n",
    "            line_list = lines[i].split(\":\")\n",
    "            if len(line_list) == 6:\n",
    "                # fig = plt.figure(figsize=(8, 4))\n",
    "                title_str = lines[i]\n",
    "                title_str = lines[i].replace('\\n', '').replace('Norm', '').replace('Feature', '').replace(':', '_').replace(',', '_').replace(' ', '')\n",
    "                # fig.suptitle(title_str)\n",
    "                \n",
    "                train_losses = np.array(lines[i+1].split(':')[1].replace('[', '').replace(']', '').replace(' ', '').replace('\\n', '').split(',')).astype(float)\n",
    "                train_accuracies = np.array(lines[i+2].split(':')[1].replace('[', '').replace(']', '').replace(' ', '').replace('\\n', '').split(',')).astype(float)\n",
    "                valid_losses = np.array(lines[i+3].split(':')[1].replace('[', '').replace(']', '').replace(' ', '').replace('\\n', '').split(',')).astype(float)\n",
    "                valid_accuracies = np.array(lines[i+4].split(':')[1].replace('[', '').replace(']', '').replace(' ', '').replace('\\n', '').split(',')).astype(float)\n",
    "                test_acc[lines[i]] = float(lines[i+5].split(':')[1])\n",
    "                test_acc_line =np.full((valid_losses.shape), test_acc[lines[i]])\n",
    "                i += 6\n",
    "        i = 0\n",
    "        max_v = 0\n",
    "        \n",
    "        fig = plt.figure(figsize=(16,4))\n",
    "        hidden_neurons = {10:'red', 100:'green', 500:'blue', 1000:'black'}\n",
    "        learning_rates = {0.01:'red', 0.001:'green', 0.0001: 'blue'}\n",
    "        batch_sizes = {1: 'red', 8: 'green', 32: 'blue', 128: 'black'}\n",
    "        n_layers = {2: 'red', 4: 'green', 6: 'blue'}\n",
    "        for net in test_acc:\n",
    "            layers = int(net.split(',')[0].split(':')[1].replace(' ', ''))\n",
    "            neurons = int(net.split(',')[1].split(':')[1].replace(' ', ''))\n",
    "            lr = float(net.split(',')[2].split(':')[1].replace(' ', ''))\n",
    "            batch_size = int(net.split(',')[3].split(':')[1].replace(' ', ''))\n",
    "            if test_acc[net] > .0:\n",
    "                plt.bar(i, (test_acc[net]), color=hidden_neurons[neurons])\n",
    "                i+=1\n",
    "            if float(test_acc[net]) > max_v:\n",
    "                max_v = test_acc[net]\n",
    "                best_net = net\n",
    "        print(\"best:\", test_acc[best_net], best_net)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_long_best:\n",
      "best: 0.8166666666666667 N_layers: 2, N_hidden_neurons: 1000, LR: 0.0001, Batch_size: 8, Epochs: 50\n",
      "\n",
      "LSTM_long_best:\n",
      "best: 0.9 N_layers: 2, N_hidden_neurons: 500, LR: 0.001, Batch_size: 8, Epochs: 50\n",
      "\n",
      "MLP_short_best:\n",
      "best: 0.9504950495049505 N_layers: 2, N_hidden_neurons: 100, LR: 0.0001, Batch_size: 8, Epochs: 50\n",
      "\n",
      "LSTM_short_best:\n",
      "best: 0.95 N_layers: 4, N_hidden_neurons: 1000, LR: 0.0001, Batch_size: 8, Epochs: 50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"MLP_long_best:\")\n",
    "find_best_net('../output/last_MLP/results_long_hyp_MLP.txt')\n",
    "\n",
    "print(\"LSTM_long_best:\")\n",
    "find_best_net('../output/results_long_hyp_LSTM.txt')\n",
    "\n",
    "print(\"MLP_short_best:\")\n",
    "find_best_net('../output/last_MLP/results_short_hyp_MLP.txt')\n",
    "\n",
    "print(\"LSTM_short_best:\")\n",
    "find_best_net('../output/results_short_hyp_LSTM.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best: 0.9504950495049505 N_layers: 2, N_hidden_neurons: 100, LR: 0.0001, Batch_size: 8, Epochs: 50\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
