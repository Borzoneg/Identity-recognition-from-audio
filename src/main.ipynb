{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "rng = np.random.default_rng(123456)\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import copy\n",
    "from scipy.io import wavfile\n",
    "import random\n",
    "\n",
    "import os\n",
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioDataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_dataset = \"short_audio_dataset\"\n",
    "sliced_dataset_lenght = 16050\n",
    "# sliced_dataset = \"shorter_audio_dataset\"\n",
    "# sliced_dataset_lenght = 4013\n",
    "original_dataset = \"audio_dataset\"\n",
    "original_dataset_lenght = 80249\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, root_path=\"./data/\", drop_both=False, use_short=False, active='wavs', normalize=False):\n",
    "        root_folder = root_path + original_dataset if not use_short else root_path + sliced_dataset\n",
    "        self.max_length = original_dataset_lenght if not use_short else sliced_dataset_lenght\n",
    "        self.class_map = {\"esben\" : 0, \"peter\": 1, \"both\": 2}\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.min_val = 10e10\n",
    "        self.max_val = 0\n",
    "        self.normalize = normalize\n",
    "        self.wavs, self.mfccs, self.chromas, self.contrasts, self.centroids, self.bandwidths = [], [], [], [], [], []\n",
    "        self.wavs_norm, self.mfccs_norm, self.chromas_norm, self.contrasts_norm, self.centroids_norm, self.bandwidths_norm = [], [], [], [], [], []\n",
    "        \n",
    "        print(\"Start reading files and genearting features\")\n",
    "        for subdir, dirs, files in os.walk(root_folder):\n",
    "            for file_name in files:\n",
    "                if drop_both and \"both\" in subdir:\n",
    "                   continue\n",
    "        \n",
    "                file_path = os.path.join(subdir, file_name)\n",
    "                self.sample_rate, wav = wavfile.read(file_path)\n",
    "                wav = wav.astype(np.float32)\n",
    "                \n",
    "                if wav.shape[0] > self.max_length:\n",
    "                    self.max_length = wav.shape[0]\n",
    "                    print(\"Found wav with more length than specified max one, new max is:\", wav.shape[0])\n",
    "                \n",
    "                self.feature_extraction(wav, self.sample_rate)\n",
    "                wav = np.pad(wav, (0, self.max_length-wav.shape[0]))\n",
    "                label_str = file_path.split('/')[-3][2:]\n",
    "                label = (np.int64(self.class_map[label_str]))\n",
    "                \n",
    "                self.max_val = np.max(wav) if np.max(wav) > self.max_val else self.max_val\n",
    "                self.min_val = np.min(wav) if np.min(wav) < self.min_val else self.min_val\n",
    "                \n",
    "                self.wavs.append(wav)\n",
    "                self.labels.append(label)\n",
    "               \n",
    "        self.wavs = np.array(self.wavs)\n",
    "        self.mu  = self.wavs.mean()\n",
    "        self.std = np.std(self.wavs)\n",
    "        # self.wavs = torch.Tensor(self.wavs)\n",
    "\n",
    "        self.active = active\n",
    "        self.values_dict = {'wavs': 0, 'mfcc': 1, 'chroma': 2, 'contrast': 3, 'centroid': 4, 'bandwidth': 5}\n",
    "        self.values_list = [self.wavs, self.mfccs, self.chromas, self.contrasts, self.centroids, self.bandwidths]\n",
    "        self.values_norm_list = []\n",
    "        print(\"Generating normalized arrays\")\n",
    "        for lst in self.values_list:\n",
    "            self.values_norm_list.append((lst + np.abs(np.min(lst))) / (np.abs(np.min(lst)) + np.max(lst)))\n",
    "            \n",
    "        print(\"=\"*40)\n",
    "        print(\"Loaded DATABASE from {}\\n{} total file\\nLongest file is {} long\\nMean: {}\\nStandard deviation: {}\\n\".\n",
    "              format(root_folder, len(self.wavs), self.max_length, self.mu, self.std))\n",
    "        print(\"=\"*40)\n",
    "\n",
    "    def feature_extraction(self, wav, sample_rate):\n",
    "        self.mfccs.append(np.transpose(np.mean(librosa.feature.mfcc(y=wav, sr=sample_rate, n_mfcc=128).T, axis=0)))\n",
    "        # self.chromas.append(np.transpose(np.mean(librosa.feature.chroma_cqt(y=wav, sr=sample_rate).T, axis=0)))\n",
    "        self.chromas.append(np.transpose(np.mean(librosa.feature.chroma_stft(y=wav, sr=sample_rate).T, axis=0)))\n",
    "        self.contrasts.append(np.transpose(np.mean(librosa.feature.spectral_contrast(y=wav, sr=sample_rate).T, axis=0)))\n",
    "        self.centroids.append(np.transpose(np.mean(librosa.feature.spectral_centroid(y=wav, sr=sample_rate).T, axis=0)))\n",
    "        self.bandwidths.append(np.transpose(np.mean(librosa.feature.spectral_bandwidth(y=wav, sr=sample_rate).T, axis=0)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wavs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        y = self.labels[idx]\n",
    "        x = self.values_list[self.values_dict[self.active]][idx]\n",
    "        if self.normalize:\n",
    "            x = self.values_norm_list[self.values_dict[self.active]][idx]\n",
    "        x = torch.Tensor(x)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start reading files and genearting features\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m audio_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mAudioDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_both\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_short\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m dataset_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(audio_dataset)\n\u001b[1;32m      3\u001b[0m train_size, test_size, valid_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(dataset_len \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.7\u001b[39m), \u001b[38;5;28mround\u001b[39m(dataset_len \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.2\u001b[39m), \u001b[38;5;28mround\u001b[39m(dataset_len \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 35\u001b[0m, in \u001b[0;36mAudioDataset.__init__\u001b[0;34m(self, root_path, drop_both, use_short, active, normalize)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m=\u001b[39m wav\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound wav with more length than specified max one, new max is:\u001b[39m\u001b[38;5;124m\"\u001b[39m, wav\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m wav \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpad(wav, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;241m-\u001b[39mwav\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     37\u001b[0m label_str \u001b[38;5;241m=\u001b[39m file_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m][\u001b[38;5;241m2\u001b[39m:]\n",
      "Cell \u001b[0;32mIn[12], line 65\u001b[0m, in \u001b[0;36mAudioDataset.feature_extraction\u001b[0;34m(self, wav, sample_rate)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeature_extraction\u001b[39m(\u001b[38;5;28mself\u001b[39m, wav, sample_rate):\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmfccs\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mtranspose(np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmfcc\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwav\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mfcc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)))\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# self.chromas.append(np.transpose(np.mean(librosa.feature.chroma_cqt(y=wav, sr=sample_rate).T, axis=0)))\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchromas\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mtranspose(np\u001b[38;5;241m.\u001b[39mmean(librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mchroma_stft(y\u001b[38;5;241m=\u001b[39mwav, sr\u001b[38;5;241m=\u001b[39msample_rate)\u001b[38;5;241m.\u001b[39mT, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/librosa/feature/spectral.py:1989\u001b[0m, in \u001b[0;36mmfcc\u001b[0;34m(y, sr, S, n_mfcc, dct_type, norm, lifter, **kwargs)\u001b[0m\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Mel-frequency cepstral coefficients (MFCCs)\u001b[39;00m\n\u001b[1;32m   1844\u001b[0m \n\u001b[1;32m   1845\u001b[0m \u001b[38;5;124;03m.. warning:: If multi-channel audio input ``y`` is provided, the MFCC\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1985\u001b[0m \u001b[38;5;124;03m>>> fig.colorbar(img2, ax=[ax[1]])\u001b[39;00m\n\u001b[1;32m   1986\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m S \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1988\u001b[0m     \u001b[38;5;66;03m# multichannel behavior may be different due to relative noise floor differences between channels\u001b[39;00m\n\u001b[0;32m-> 1989\u001b[0m     S \u001b[38;5;241m=\u001b[39m power_to_db(\u001b[43mmelspectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1991\u001b[0m M: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39mfftpack\u001b[38;5;241m.\u001b[39mdct(S, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mdct_type, norm\u001b[38;5;241m=\u001b[39mnorm)[\n\u001b[1;32m   1992\u001b[0m     \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :n_mfcc, :\n\u001b[1;32m   1993\u001b[0m ]\n\u001b[1;32m   1995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lifter \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1996\u001b[0m     \u001b[38;5;66;03m# shape lifter for broadcasting\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/librosa/feature/spectral.py:2143\u001b[0m, in \u001b[0;36mmelspectrogram\u001b[0;34m(y, sr, S, n_fft, hop_length, win_length, window, center, pad_mode, power, **kwargs)\u001b[0m\n\u001b[1;32m   2130\u001b[0m S, n_fft \u001b[38;5;241m=\u001b[39m _spectrogram(\n\u001b[1;32m   2131\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m   2132\u001b[0m     S\u001b[38;5;241m=\u001b[39mS,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2139\u001b[0m     pad_mode\u001b[38;5;241m=\u001b[39mpad_mode,\n\u001b[1;32m   2140\u001b[0m )\n\u001b[1;32m   2142\u001b[0m \u001b[38;5;66;03m# Build a Mel filter\u001b[39;00m\n\u001b[0;32m-> 2143\u001b[0m mel_basis \u001b[38;5;241m=\u001b[39m \u001b[43mfilters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2145\u001b[0m melspec: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...ft,mf->...mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, S, mel_basis, optimize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m melspec\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/librosa/filters.py:252\u001b[0m, in \u001b[0;36mmel\u001b[0;34m(sr, n_fft, n_mels, fmin, fmax, htk, norm, dtype)\u001b[0m\n\u001b[1;32m    249\u001b[0m     weights \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mnormalize(weights, norm\u001b[38;5;241m=\u001b[39mnorm, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# Only check weights if f_mel[0] is positive\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall((mel_f[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m|\u001b[39m (\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)):\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;66;03m# This means we have an empty channel somewhere\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmpty filters detected in mel frequency basis. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome channels will produce empty responses. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    259\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    260\u001b[0m     )\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weights\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/_methods.py:39\u001b[0m, in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     33\u001b[0m     _complex_to_float\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[1;32m     34\u001b[0m         nt\u001b[38;5;241m.\u001b[39mdtype(nt\u001b[38;5;241m.\u001b[39mclongdouble) : nt\u001b[38;5;241m.\u001b[39mdtype(nt\u001b[38;5;241m.\u001b[39mlongdouble),\n\u001b[1;32m     35\u001b[0m     })\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# avoid keyword arguments to speed up parsing, saves about 15%-20% for very\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# small reductions\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_amax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_maximum(a, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out, keepdims, initial, where)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_amin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     44\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "audio_dataset = AudioDataset(root_path=\"../data/\", drop_both=False, use_short=False)\n",
    "dataset_len = len(audio_dataset)\n",
    "train_size, test_size, valid_size = round(dataset_len * 0.7), round(dataset_len * 0.2), round(dataset_len * 0.1)\n",
    "\n",
    "dataset_train, dataset_test, dataset_valid = torch.utils.data.random_split(audio_dataset, (train_size, test_size, valid_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'batch_size': 1, 'num_workers': 1}\n",
    "loader_train = torch.utils.data.DataLoader(dataset_train, **kwargs, shuffle=True)\n",
    "loader_test = torch.utils.data.DataLoader(dataset_test, **kwargs, shuffle=True)\n",
    "loader_valid = torch.utils.data.DataLoader(dataset_valid, **kwargs, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size, test_size = round(dataset_len * 0.8), round(dataset_len * 0.2)\n",
    "dataset_train, dataset_test = torch.utils.data.random_split(audio_dataset, (train_size, test_size))\n",
    "kwargs = {'batch_size': 1, 'num_workers': 1}\n",
    "loader_train = torch.utils.data.DataLoader(dataset_train, **kwargs, shuffle=True)\n",
    "loader_test = torch.utils.data.DataLoader(dataset_test, **kwargs, shuffle=True)\n",
    "train_data = []\n",
    "train_labels = []\n",
    "test_data = []\n",
    "test_labels = []\n",
    "lists_data = [train_data, test_data]\n",
    "lists_labels = [train_labels, test_labels]\n",
    "\n",
    "i = 0\n",
    "for loader in [loader_train, loader_test]:\n",
    "    for x, y in loader:\n",
    "        lists_data[i].append(np.array(x[0]))\n",
    "        lists_labels[i].append(np.array(y[0]))\n",
    "    i += 1\n",
    "for li in lists_data:\n",
    "    print(len(li))\n",
    "for li in lists_labels:\n",
    "    print(len(li))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dataset.active = 'wavs'\n",
    "audio_dataset.normalize = False\n",
    "\n",
    "for x, y in loader_test:\n",
    "    plt.plot(np.arange(x.shape[1]), x.flatten())\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_len):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_len, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, input_len),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.decode(self.encode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(len(dataset_train[0][0]))\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(25):\n",
    "    print(f'Epoch {epoch+1:02}/25', end=' ')\n",
    "    for x, _ in loader_train:  \n",
    "        x_rec = model(x) \n",
    "        loss = F.binary_cross_entropy(x_rec, x)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        mse = F.mse_loss(x_rec, x)\n",
    "        mae = F.l1_loss(x_rec, x)\n",
    "        \n",
    "    print(f'loss: {loss.item():.4f} - rmse: {np.sqrt(mse.item()):.4f} - mae: {mae.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_param_search(train_data, train_labels, test_data, test_labels,\n",
    "                     metrics=('manhattan', 'euclidean', 'chebyshev'), \n",
    "                     ks=(1, 3, 5, 10, 25, 50, 100, 250), \n",
    "                     n_train=None, n_test=None, algorithm='brute'):\n",
    "  \"\"\"\n",
    "  Takes a dataset and plots knn classification accuracy \n",
    "  for different hyper parameters.\n",
    "\n",
    "  n_train and n_test allows to subsample the dataset for faster iteration\n",
    "  \"\"\"\n",
    "  x_train = np.array(train_data)\n",
    "  y_train = np.array(train_labels)\n",
    "  x_test = np.array(test_data)\n",
    "  y_test = np.array(test_labels)\n",
    "  max_acc = 0\n",
    "  datas = []\n",
    "  for metric in metrics:\n",
    "    for k in ks:\n",
    "        print(f'Metric: {metric}; k: {k:3};', end=' ')\n",
    "        classifier = neighbors.KNeighborsClassifier(k, algorithm=algorithm, metric=metric)\n",
    "        classifier = classifier.fit(x_train, y_train)\n",
    "\n",
    "        labels = classifier.predict(x_test)\n",
    "        \n",
    "        correct = labels == np.array(y_test)\n",
    "        print(f'Accuracy: {correct.mean() * 100:.2f}%')\n",
    "        if correct.mean() > max_acc:\n",
    "          max_acc = correct.mean()\n",
    "          best_classifier = classifier\n",
    "          best_metric = metric\n",
    "          best_k = k\n",
    "        datas.append([metric, k, correct.mean()])\n",
    "      \n",
    "  print(f'Best classifier | metric: {best_metric}; k: {best_k:3}; accuracy: {max_acc * 100:.2f}%')\n",
    "  return best_classifier, datas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/knn_results.txt', 'w') as f:\n",
    "  for norm_opt in [False, True]:\n",
    "    audio_dataset.normalize = norm_opt\n",
    "    for feature in ['wavs', 'mfcc', 'chroma', 'contrast']:\n",
    "      audio_dataset.active = feature\n",
    "      \n",
    "      train_data = []\n",
    "      train_labels = []\n",
    "      test_data = []\n",
    "      test_labels = []\n",
    "      lists_data = [train_data, test_data]\n",
    "      lists_labels = [train_labels, test_labels]\n",
    "      i = 0\n",
    "      for loader in [loader_train, loader_test]:\n",
    "          for x, y in loader:\n",
    "              lists_data[i].append(np.array(x[0]))\n",
    "              lists_labels[i].append(np.array(y[0]))\n",
    "          i += 1\n",
    "\n",
    "      classifier, datas = knn_param_search(train_data, \n",
    "                                    train_labels, \n",
    "                                    test_data,\n",
    "                                    test_labels,\n",
    "                                  #   metrics=['euclidean'],\n",
    "                                    ks=(1, 3, 5, 10, 25, 50, 100))\n",
    "      f.write(f'Norm:{norm_opt};feature:{feature};{datas}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old plot KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_norm_no_feat = [43.00, 50.00, 50.00, 50.00, 51.00, 51.00, 50.00, 50.00, 50.00, 47.00, 49.00, 50.00, 50.00, 50.00, 50.00, 50.00, 45.00, 48.00, 49.00, 54.00, 50.00, 53.00, 54.00, 52.00]\n",
    "norm_no_feat = [50.00, 49.00, 58.00, 57.00, 59.00, 54.00, 57.00, 59.00, 52.00, 54.00, 57.00, 57.00, 45.00, 57.00, 57.00, 57.00, 46.00, 45.00, 47.00, 44.00, 51.00, 59.00, 59.00, 60.00]\n",
    "norm_feat = [77.00, 78.00, 72.00, 68.00, 71.00, 73.00, 70.00, 58.00, 78.00, 78.00, 76.00, 78.00, 78.00, 81.00, 76.00, 64.00, 81.00, 85.00, 84.00, 81.00, 81.00, 73.00, 74.00, 75.00]\n",
    "no_norm_feat = [88.00, 90.00, 93.00, 88.00, 84.00, 85.00, 83.00, 80.00, 83.00, 89.00, 87.00, 82.00, 83.00, 81.00, 77.00, 70.00, 77.00, 76.00, 76.00, 80.00, 76.00, 75.00, 71.00, 61.00]\n",
    "datas = [no_norm_no_feat ,norm_no_feat ,norm_feat ,no_norm_feat]\n",
    "titles = ['Norm=False; Features=False' ,'Norm=True; Features=False', 'Norm=True; Features=True', 'Norm=False; Features=True']\n",
    "ks = [1, 3, 5, 10, 25, 50, 100, 250]\n",
    "metrics = ['manhattan', 'euclidean', 'chebyshev']\n",
    "\n",
    "fig = plt.figure(figsize=(28, 16))\n",
    "for i, data in enumerate(datas):\n",
    "    # fig = plt.figure(figsize=(32,4))\n",
    "    ax = fig.add_subplot(2, 2, i+1)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    ax.title.set_text(titles[i])\n",
    "    tmp = []\n",
    "    for j in range(len(ks)):\n",
    "        tmp.append(data[j])\n",
    "        tmp.append(data[j+len(ks)])\n",
    "        tmp.append(data[j+len(ks)*2])\n",
    "        tmp.append(0)\n",
    "    tmp.pop()\n",
    "    clrs = ('blue', 'red', 'green') * len(ks)\n",
    "\n",
    "    bars = plt.bar(range(len(tmp)), tmp, width=0.8, color=clrs)\n",
    "    for bar in bars:\n",
    "        if bar.get_height() > 0:\n",
    "            ax.text(bar.get_x(), bar.get_height(), format(int(bar.get_height())))\n",
    "\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_ylabel('accuracy')\n",
    "\n",
    "    ax.set_xticks(range(len(tmp)))\n",
    "    x_tick_labels = []\n",
    "    for k in ks:\n",
    "        x_tick_labels.append('')\n",
    "        x_tick_labels.append(format(k))\n",
    "        x_tick_labels.append('')\n",
    "        x_tick_labels.append('')\n",
    "    x_tick_labels.pop()\n",
    "    ax.set_xticklabels(x_tick_labels)\n",
    "    ax.set_xlabel('K')\n",
    "\n",
    "    if i == 0:\n",
    "        legend_patches = [mpatches.Patch(color=clrs[0], label=metrics[0]), mpatches.Patch(color=clrs[1], label=metrics[1]), mpatches.Patch(color=clrs[2], label=metrics[2])]\n",
    "        plt.legend(handles=legend_patches)\n",
    "    # plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by extending nn.Module:\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self, n_hidden_neurons: int, input_len):\n",
    "    super().__init__()\n",
    "    self.fc1 = nn.Linear(input_len, n_hidden_neurons)\n",
    "    self.fc2 = nn.Linear(n_hidden_neurons, 2)\n",
    "\n",
    "  def forward(self, x: torch.Tensor):\n",
    "    # x = x.flatten(start_dim=0)  # (N, d_data)\n",
    "    h = F.relu(self.fc1(x))\n",
    "    logits = self.fc2(h)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses, all_accuracies = {}, {}\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "learning_rate = 5e-2\n",
    "n_epochs = 10\n",
    "\n",
    "for n_hidden_neurons in ([10, 100, 1000, 10000]):\n",
    "  print(f\"Hidden neurons: {n_hidden_neurons}\")\n",
    "  model = MLP(n_hidden_neurons, len(dataset_train[0][0]))\n",
    "  opt = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  train_losses, train_accuracies = [], []\n",
    "  valid_losses, valid_accuracies = [], []\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "    print(f\"{epoch+1:02} / 10 Epoch\", end=\": \")\n",
    "\n",
    "    epoch_losses = []\n",
    "    correct, total = 0, 0\n",
    "    for x, y in loader_train:\n",
    "      opt.zero_grad()\n",
    "  \n",
    "      logits = model(x)  # logits: common name for the output before softmax activation\n",
    "  \n",
    "      log_probs = F.log_softmax(logits, dim=1)  # numerically stable version of log(softmax(logits))\n",
    "  \n",
    "      loss = F.nll_loss(log_probs, y)  # negative log likelihood loss\n",
    "      # or just: loss = F.cross_entropy(logits, y)\n",
    "\n",
    "      loss.backward()\n",
    "      opt.step()\n",
    "      epoch_losses.append(loss.item())\n",
    "      total += len(x)\n",
    "      correct += (torch.argmax(logits, dim=1) == y).sum().item()\n",
    "    train_losses.append(np.mean(epoch_losses))\n",
    "    train_accuracies.append(correct / total)\n",
    "    print(f\"Train; Loss mean: {np.mean(epoch_losses):2.2f}; Accuracy: {correct / total:2.2f}\", end=\" | \")\n",
    "    \n",
    "    epoch_losses = []\n",
    "    correct, total = 0, 0\n",
    "    for x, y in loader_valid:\n",
    "      with torch.no_grad():\n",
    "        logits = model(x)\n",
    "      loss = F.cross_entropy(logits, y)\n",
    "\n",
    "      epoch_losses.append(loss.item())\n",
    "      total += len(x)\n",
    "      correct += (torch.argmax(logits, dim=1) == y).sum().item()\n",
    "    valid_losses.append(np.mean(epoch_losses))\n",
    "    valid_accuracy = correct / total\n",
    "    valid_accuracies.append(valid_accuracy)\n",
    "    print(f\"Valid; Loss mean: {np.mean(epoch_losses):2.2f}; Accuracy: {valid_accuracy:2.2f}\")\n",
    "    if valid_accuracy > best_accuracy:\n",
    "      best_accuracy = valid_accuracy\n",
    "      best_model = copy.deepcopy(model), n_hidden_neurons, epoch\n",
    "\n",
    "  all_losses[n_hidden_neurons] = train_losses, valid_losses\n",
    "  all_accuracies[n_hidden_neurons] = train_accuracies, valid_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results from MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "for n, (train_losses, valid_losses) in all_losses.items():\n",
    "  p = plt.plot(train_losses, label=f'{n}:train')\n",
    "  plt.plot(valid_losses, label=f'{n}:valid', ls='--', c=p[0].get_color())\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(0, 1)\n",
    "for n, (train_accuracies, valid_accuracies) in all_accuracies.items():\n",
    "  p = plt.plot(train_accuracies, label=f'{n}:train')\n",
    "  plt.plot(valid_accuracies, label=f'{n}:valid', ls='--', c=p[0].get_color())\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "model, n_hidden_neurons, epoch = best_model\n",
    "print(f'best val accuracy: {best_accuracy:.2f} with {n_hidden_neurons} hidden neurons after {epoch} epoch')\n",
    "\n",
    "correct, total = 0, 0\n",
    "for x, y in loader_valid:\n",
    "  with torch.no_grad():\n",
    "    logits = model(x)\n",
    "  total += len(x)\n",
    "  correct += (torch.argmax(logits, dim=1) == y).sum().item()\n",
    "print(f'test accuracy: {correct / total:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM MIMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineClassifier(nn.Module):\n",
    "    def __init__(self, input_size, n_lstm_layers=1, dropout=0.):\n",
    "        super().__init__()\n",
    "        # self.emb = nn.Embedding(128, d)  # max(emails_ascii) < 128\n",
    "        self.lstm = nn.LSTM(input_size, input_size, n_lstm_layers, batch_first=False, dropout=dropout)\n",
    "        self.lin = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x, end_idx):  # (B, nx)\n",
    "        # x = self.emb(x)  # (B, nx, d)\n",
    "        # y = self.lstm(x)[0][torch.arange(len(x)), end_idx]  # (B, d)\n",
    "        y = self.lstm(x)\n",
    "        return y\n",
    "        # return self.lin(y).view(-1)  # (B,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "model = OnlineClassifier(audio_dataset.max_length).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "sched = torch.optim.lr_scheduler.MultiStepLR(opt, (30,))\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "lrs = []\n",
    "epochs = 40\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"{epoch+1:02} / 10 Epoch\", end=\": \")\n",
    "    # train\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct = total = 0\n",
    "    for x, end_idx, y in loader_train:\n",
    "        x, end_idx, y = x.to(device), end_idx.to(device), y.to(device)\n",
    "        logits = model(x, end_idx)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, y.float())\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        correct += ((torch.sigmoid(logits) > 0.5) == y).sum().item()\n",
    "        total += len(x)\n",
    "    train_loss = np.mean(losses)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # valid\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct = total = 0\n",
    "    for x, end_idx, y in loader_valid:\n",
    "        x, end_idx, y = x.to(device), end_idx.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x, end_idx)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, y.float())\n",
    "        losses.append(loss.item())\n",
    "        correct += ((torch.sigmoid(logits) > 0.5) == y).sum().item()\n",
    "        total += len(x)\n",
    "    valid_loss = np.mean(losses)\n",
    "    valid_acc = correct / total\n",
    "\n",
    "    # sched\n",
    "    sched.step()\n",
    "    \n",
    "    # history\n",
    "    lrs.append(next(iter(opt.param_groups))['lr'])\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    valid_accuracies.append(valid_acc)\n",
    "\n",
    "    print(f'loss: {train_loss:.3f}/{valid_loss:.3f}, acc: {train_acc:.2f}/{valid_acc:.2f}')\n",
    "\n",
    "# plot history\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 3))\n",
    "axs[0].plot(train_losses, label='train')\n",
    "axs[0].plot(valid_losses, label='valid')\n",
    "axs[0].set_ylabel('loss')\n",
    "axs[0].legend()\n",
    "axs[1].plot(train_accuracies, label='train')\n",
    "axs[1].plot(valid_accuracies, label='valid')\n",
    "axs[1].set_ylabel('acc')\n",
    "axs[1].set_ylim(0.8, 1)\n",
    "axs[1].legend()\n",
    "axs[2].plot(lrs)\n",
    "axs[2].set_ylabel('lr')\n",
    "axs[2].set_yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot from KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_function_knn(file_name, folder_name=None, showing=True):\n",
    "    if not showing:\n",
    "        print(\"Plotting is off\")\n",
    "        plt.ioff()\n",
    "    if folder_name == None:\n",
    "        folder_name = '../output/' + file_name.split('/')[-1].replace('.txt', '')\n",
    "    folder_name += '_' + str(round(random.random()*1e4)) + '/'    \n",
    "    width = 0.8\n",
    "    if 'long' in file_name:\n",
    "        ks=(1, 3, 5, 10, 25, 50, 100)\n",
    "    else:\n",
    "        ks=(1, 3, 5, 10, 25, 50, 100, 250)\n",
    "    metrics = ['manhattan', 'euclidean', 'chebyshev']\n",
    "    color_dict = {'manhattan':'firebrick', 'euclidean':'dodgerblue', 'chebyshev':'seagreen'}\n",
    "    os.mkdir(folder_name)\n",
    "\n",
    "    with open(file_name) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            norm_str = line.split(';')[0]\n",
    "            feature_str = line.split(';')[1]\n",
    "            data_list = line.split(';')[2].replace('[', '').replace(']', '').replace('\\'', '').replace(' ', '').split(',')\n",
    "            fig = plt.figure(figsize=(10, 5))\n",
    "            fig.suptitle(norm_str + ' ' + feature_str)\n",
    "            ax = fig.add_subplot()\n",
    "\n",
    "            fig.tight_layout()\n",
    "            i = 0\n",
    "            x_pos = {1: 0, 3: 3, 5: 6, 10: 9, 25: 12, 50: 15, 100: 18, 250:21}\n",
    "            while i < (len(data_list) -2):\n",
    "                metric = data_list[i]\n",
    "                k = int(data_list[i+1])\n",
    "                acc = float(data_list[i+2])\n",
    "                i += 3\n",
    "                plt.bar(x_pos[k], acc-0.01, width=width, color=color_dict[metric])\n",
    "                ax.text(x_pos[k]-0.5, acc, '{0:.2f}'.format(acc))\n",
    "                x_pos[k] += 1\n",
    "                \n",
    "            x_tick_labels = []\n",
    "            for k in ks:\n",
    "                x_tick_labels.append('')\n",
    "                x_tick_labels.append(format(k))\n",
    "                x_tick_labels.append('')\n",
    "                \n",
    "            ax.set_yticks(np.arange(0, 1, 0.1))\n",
    "            ax.set_xticks(range(len(x_tick_labels)))\n",
    "            ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "            ax.set_ylabel('accuracy')\n",
    "            ax.set_xlabel('K')\n",
    "\n",
    "            legend_patches = []\n",
    "            for metric in color_dict:\n",
    "                legend_patches.append(mpatches.Patch(color=color_dict[metric], label=metric))\n",
    "            plt.legend(handles=legend_patches)\n",
    "            plt.savefig(folder_name + norm_str.replace(':', '_') + '_' + feature_str.replace(':', '_') + '.png')\n",
    "            if showing:\n",
    "                plt.show()\n",
    "            plt.clf()\n",
    "    if not showing:\n",
    "        plt.ion()\n",
    "    plt.close(\"all\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting function LSTM/MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_function_lstm_mlp(file_name, folder_name=None, showing=True):\n",
    "    if not showing:\n",
    "        print(\"Plotting is off\")\n",
    "        plt.ioff()\n",
    "    if folder_name == None:\n",
    "        folder_name = '../output/' + file_name.split('/')[-1].replace('.txt', '')\n",
    "    folder_name += '_' + str(round(random.random()*1e4)) + '/'\n",
    "    test_acc = {}\n",
    "    os.mkdir(folder_name)\n",
    "    with open(file_name) as  f:\n",
    "        lines = f.readlines()\n",
    "        for i in range(len(lines)):\n",
    "            line_list = lines[i].split(\":\")\n",
    "            if len(line_list) == 3:\n",
    "                fig = plt.figure(figsize=(8, 4))\n",
    "                title_str = lines[i].replace('\\n', '').replace('Norm', '').replace('Feature', '').replace(':', '').replace(',', '_').replace(' ', '')\n",
    "                fig.suptitle(title_str)\n",
    "                \n",
    "                train_losses = np.array(lines[i+1].split(':')[1].replace('[', '').replace(']', '').replace(' ', '').replace('\\n', '').split(',')).astype(float)\n",
    "                train_accuracies = np.array(lines[i+2].split(':')[1].replace('[', '').replace(']', '').replace(' ', '').replace('\\n', '').split(',')).astype(float)\n",
    "                valid_losses = np.array(lines[i+3].split(':')[1].replace('[', '').replace(']', '').replace(' ', '').replace('\\n', '').split(',')).astype(float)\n",
    "                valid_accuracies = np.array(lines[i+4].split(':')[1].replace('[', '').replace(']', '').replace(' ', '').replace('\\n', '').split(',')).astype(float)\n",
    "                test_acc[lines[i]] = float(lines[i+5].split(':')[1])\n",
    "                test_acc_line =np.full((valid_losses.shape), test_acc[lines[i]])\n",
    "                i += 5\n",
    "                \n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.xlabel('epoch')\n",
    "                plt.ylabel('loss')\n",
    "                plt.plot(train_losses,  label='train', c='blue')\n",
    "                plt.plot(valid_losses,  label='valid', c='green')\n",
    "                plt.legend()\n",
    "                plt.grid()\n",
    "\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.xlabel('epoch')\n",
    "                plt.ylabel('accuracy')\n",
    "                plt.ylim(0, 1)\n",
    "                plt.yticks(np.linspace(0, 1, 10))\n",
    "                \n",
    "                plt.plot(train_accuracies, label='train', c='blue')\n",
    "                plt.plot(valid_accuracies, label='valid', c='green')\n",
    "                plt.plot(test_acc_line, label='test', ls='--', c='red')\n",
    "                plt.legend()\n",
    "                plt.grid()\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(folder_name + title_str + '.png')\n",
    "                if showing:\n",
    "                    print('showing')\n",
    "                    plt.show()\n",
    "\n",
    "        fig = plt.figure(figsize=(10,5))\n",
    "        fig.suptitle('Test accuracy comparison')\n",
    "        \n",
    "        clrs = clrs = plt.cm.rainbow(np.linspace(0, 1, 8))\n",
    "\n",
    "        ax = fig.add_subplot()\n",
    "        bars = plt.bar(range(len(test_acc)), list(test_acc.values()), hatch=['/', '/', '/', '/', '', '', '', ''], width=0.8, color=clrs)\n",
    "        \n",
    "        xlabels = []\n",
    "        for bar, feature in zip(bars, test_acc):\n",
    "            if bar.get_height() > 0:\n",
    "                feature_string = feature.replace(':','').replace(', Feature', '').replace(' ', '').replace('Norm', '').replace('False', '').replace('True', '')\n",
    "                if 'False' in feature_string:\n",
    "                    feature_string = feature_string.replace('Norm', '').replace('False', '')\n",
    "                else:\n",
    "                    feature_string = feature_string.replace('True', '')\n",
    "                ax.text(bar.get_x(), bar.get_height() + 0.01, \"{0:.5f}\".format(bar.get_height()))\n",
    "                xlabels.append(feature_string.capitalize())\n",
    "        \n",
    "        ax.set_yticks(np.arange(0, 1, 0.1))\n",
    "        ax.set_xticks(range(len(xlabels)))\n",
    "        ax.set_xticklabels(xlabels)\n",
    "        \n",
    "        legend_patches = [mpatches.Patch(hatch='///', edgecolor='black', fill=False, label='normalized')]\n",
    "        plt.legend(handles=legend_patches)\n",
    "        \n",
    "        plt.savefig(folder_name + 'test_acc.png')\n",
    "        if showing:\n",
    "            plt.show()\n",
    "        if not showing:\n",
    "            plt.ion()\n",
    "        plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting is off\n",
      "Plotting is off\n",
      "Plotting is off\n",
      "Plotting is off\n",
      "Plotting is off\n"
     ]
    }
   ],
   "source": [
    "plot_function_knn('../output/results_short_both_KNN.txt', folder_name=None, showing=False)\n",
    "plot_function_knn('../output/results_long_both_KNN.txt', folder_name=None, showing=False)\n",
    "plot_function_lstm_mlp('../output/results_short_MLP.txt', folder_name=None, showing=False)\n",
    "plot_function_lstm_mlp('../output/results_long_both_MLP.txt', folder_name=None,showing=False)\n",
    "plot_function_lstm_mlp('../output/results_long_both_LSTM.txt', folder_name=None,showing=False)\n",
    "# plot_function_lstm_mlp('../output/results_short_hyp_MLP.txt', folder_name=None, showing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best: 0.85 N_layers: 2, N_hidden_neurons: 100, LR: 0.001, Batch_size: 1, Epochs: 50\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_name = '../output/results_long_hyp_MLP.txt'\n",
    "# folder_name += '_' + str(round(random.random()*1e4)) + '/'\n",
    "test_acc = {}\n",
    "# os.mkdir(folder_name)\n",
    "with open(file_name) as  f:\n",
    "    lines = f.readlines()\n",
    "    for i in range(len(lines)):\n",
    "        line_list = lines[i].split(\":\")\n",
    "        if len(line_list) == 6:\n",
    "            # fig = plt.figure(figsize=(8, 4))\n",
    "            title_str = lines[i]\n",
    "            title_str = lines[i].replace('\\n', '').replace('Norm', '').replace('Feature', '').replace(':', '_').replace(',', '_').replace(' ', '')\n",
    "            # fig.suptitle(title_str)\n",
    "            \n",
    "            train_losses = np.array(lines[i+1].split(':')[1].replace('[', '').replace(']', '').replace(' ', '').replace('\\n', '').split(',')).astype(float)\n",
    "            train_accuracies = np.array(lines[i+2].split(':')[1].replace('[', '').replace(']', '').replace(' ', '').replace('\\n', '').split(',')).astype(float)\n",
    "            valid_losses = np.array(lines[i+3].split(':')[1].replace('[', '').replace(']', '').replace(' ', '').replace('\\n', '').split(',')).astype(float)\n",
    "            valid_accuracies = np.array(lines[i+4].split(':')[1].replace('[', '').replace(']', '').replace(' ', '').replace('\\n', '').split(',')).astype(float)\n",
    "            test_acc[lines[i]] = float(lines[i+5].split(':')[1])\n",
    "            test_acc_line =np.full((valid_losses.shape), test_acc[lines[i]])\n",
    "            i += 6\n",
    "    i = 0\n",
    "    max_v = 0\n",
    "    fig = plt.figure(figsize=(16,4))\n",
    "    hidden_neurons = {10:'red', 100:'green', 500:'blue', 1000:'black'}\n",
    "    learning_rates = {0.01:'red', 0.001:'green', 0.0001: 'blue'}\n",
    "    batch_sizes = {1: 'red', 8: 'green', 32: 'blue', 128: 'black'}\n",
    "    n_layers = {2: 'red', 4: 'green', 6: 'blue'}\n",
    "    for net in test_acc:\n",
    "        layers = int(net.split(',')[0].split(':')[1].replace(' ', ''))\n",
    "        neurons = int(net.split(',')[1].split(':')[1].replace(' ', ''))\n",
    "        lr = float(net.split(',')[2].split(':')[1].replace(' ', ''))\n",
    "        batch_size = int(net.split(',')[3].split(':')[1].replace(' ', ''))\n",
    "        if test_acc[net] > .0:\n",
    "            plt.bar(i, (test_acc[net]), color=hidden_neurons[neurons])\n",
    "            i+=1\n",
    "        if float(test_acc[net]) > max_v:\n",
    "            max_v = test_acc[net]\n",
    "            best_net = net\n",
    "    print(\"best:\", test_acc[best_net], best_net)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
